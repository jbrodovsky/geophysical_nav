{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bathymetric particle filter \n",
    "\n",
    "This notebook is a testing and development environment for the geophysical particle filter. As of 30 December 2024 the initial implementation will be a velocity-state based implementation that does not do the proposed full state mechanization. The full state mechanization will be implemented in the future under a ROS2 and Gazebo simulation environment that allows for more realistic (and reliable) IMU simulation.\n",
    "\n",
    "This notebook is meant for interactive testing and development and should not be used for large-scale simulations. The intended workflow is to use the notebook to test and develop the particle filter on a small subsample of test data, then build and install it in the local virtual environment for use in the full simulation.\n",
    "\n",
    "This full simulation should then be written in the `/scripts` folder with result data saved off there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters verification\n",
    "\n",
    "First we need to tune the particle filter propagation noise to be similar to that of a marine-grade inertial navigation system. A low-end marine-grade INS should have a drift of 1 nm per 24 hours. Our typical time interval is 60 seconds and the noise parameters for velocities states need to be tuned based on this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.geophysical.particle_filter import propagate_ned, rmse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_velocity_loop(u: np.ndarray, noise: np.ndarray, dt: float = 60.0, time: int = 24 * 60) -> float:\n",
    "    P = np.asarray([[0, 0, 0, u[0], u[1], u[2]]])\n",
    "    T = P.copy()\n",
    "    t = 0\n",
    "    while t < time:\n",
    "        P = propagate_ned(P, u, dt)  # noise=np.diag(noise), noise_calibration_mode=True)\n",
    "        # Add zero-mean noise to the particles\n",
    "        P += np.random.multivariate_normal(np.zeros(6), np.diag(noise))\n",
    "        T = propagate_ned(T, u, dt)  # noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "        t += 1\n",
    "    error = rmse(P, T, include_altitude=True)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 24 * 60  # minutes\n",
    "vel_noise = 1.75  # m/s\n",
    "noise = np.array([0, 0, 0, vel_noise, vel_noise, 0.01])\n",
    "bound = 1852  # meters\n",
    "errors = []\n",
    "errors_dict = {}\n",
    "for i in range(1, 26):  # 1 to 25 nautical miles per hour\n",
    "    print(f\"Running for {i} knots\")\n",
    "    v = i * bound / 3600  # convert to m/s\n",
    "    for a in tqdm(range(1000)):\n",
    "        # Eastward\n",
    "        u = np.asarray([0.0, v, 0.0])\n",
    "        errors.append(run_velocity_loop(u, noise))\n",
    "        # Northward\n",
    "        u = np.asarray([v, 0, 0])\n",
    "        errors.append(run_velocity_loop(u, noise))\n",
    "        # Northeastward\n",
    "        u = np.array([1, 1, 0]) / np.linalg.norm([1, 1, 0])\n",
    "        u *= v\n",
    "        errors.append(run_velocity_loop(u, noise))\n",
    "    errors_dict[i] = errors.copy()\n",
    "    print(f\"Mean RMSE: {np.mean(errors): 0.2f} meters\")\n",
    "print(f\"Overall RMSE: {np.mean(errors)}\")\n",
    "# Plot the previous\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.violinplot(errors_dict.values(), showmeans=True, showmedians=True)\n",
    "plt.axhline(y=1852, color=\"k\", linestyle=\"--\", label=\"1 Nautical Mile\")\n",
    "plt.axhline(np.mean(errors), color=\"r\", linestyle=\"-\", label=\"Mean Error\")\n",
    "plt.xlabel(\"Velocity (knots)\")\n",
    "plt.ylabel(\"Error (meters)\")\n",
    "plt.title(\"Velocity Noise Tuning Error Distributions\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(\"velocity_noise_tuning.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalized noise model: $\\begin{bmatrix} 0 & 0 & 0 & 1.75 & 1.75 & 0.01 \\end{bmatrix}$\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop measurement model\n",
    "\n",
    "Next we need to develop the measurement value standard deviation. We'll first do some general examination of the data. Namely, investigating the sensor measurements to see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management import dbmgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbmgr.DatabaseManager(\".db\")\n",
    "summary = db.get_all_trajectories()\n",
    "summary = summary.drop(summary.index[-1])\n",
    "bathy_trajectories = summary[summary[\"depth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.geophysical import gmt_toolbox as gmt\n",
    "from tqdm import tqdm\n",
    "\n",
    "bathy_differences = np.empty((0,))\n",
    "\n",
    "for id in tqdm(bathy_trajectories[\"id\"]):\n",
    "    data = db.get_trajectory(id)\n",
    "    min_lon = data[\"lon\"].min()\n",
    "    max_lon = data[\"lon\"].max()\n",
    "    min_lat = data[\"lat\"].min()\n",
    "    max_lat = data[\"lat\"].max()\n",
    "    # min_lon, min_lat, max_lon, max_lat = gmt.inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    try:\n",
    "        bathy_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.BATHYMETRY,\n",
    "            gmt.ReliefResolution.FIFTEEN_SECONDS,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.25,\n",
    "        )\n",
    "    except AssertionError:\n",
    "        if min_lat == max_lat:\n",
    "            min_lat -= 0.25\n",
    "            max_lat += 0.25\n",
    "        if min_lon == max_lon:\n",
    "            min_lon -= 0.25\n",
    "            max_lon += 0.25\n",
    "        bathy_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.BATHYMETRY,\n",
    "            gmt.ReliefResolution.FIFTEEN_SECONDS,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.1,\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Failed to get bathymetry map for {id}: {min_lon}, {max_lon}, {min_lat}, {max_lat}\")\n",
    "    # d_bathy = np.hstack([d_bathy, data[\"DEPTH\"] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "    bathy_differences = np.append(\n",
    "        bathy_differences, data[\"depth\"] - (-bathy_map.get_map_point(data[\"lon\"], data[\"lat\"]))\n",
    "    )\n",
    "\n",
    "bathy_mean_d = np.mean(bathy_differences, where=~np.isnan(bathy_differences))\n",
    "bathy_std = np.std(bathy_differences, where=~np.isnan(bathy_differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def plot_measurement_statistics(\n",
    "    data: np.ndarray, title: str, xlabel: str, ylabel: str, filename: str, bin_count: int = 1000, xlims: tuple = None\n",
    "):\n",
    "    Mean = np.mean(data, where=~np.isnan(data))\n",
    "    Std = np.std(data, where=~np.isnan(data))\n",
    "    Median = np.median(data[~np.isnan(data)])\n",
    "    Mode = mode(data[~np.isnan(data)]).mode\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(data, bins=bin_count, density=True, alpha=0.75)\n",
    "    plt.axvline(Mean, color=\"r\", linestyle=\"--\", label=f\"Mean={Mean:0.2f}\")\n",
    "    plt.axvline(Mean + Std, color=\"k\", linestyle=\"--\", label=f\"$\\pm\\sigma={Std:0.2f}$\")\n",
    "    plt.axvline(Mean - Std, color=\"k\", linestyle=\"--\")\n",
    "    plt.axvline(Median, color=\"g\", linestyle=\"--\", label=f\"Median={Median:0.2f}\")\n",
    "    plt.axvline(Mode, color=\"b\", linestyle=\"--\", label=f\"Mode={Mode:0.2f}\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xlims is not None:\n",
    "        plt.xlim(xlims)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above, let's plot the data and see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_measurement_statistics(\n",
    "    bathy_differences,\n",
    "    \"Raw Bathymetry Measurement Statistics\",\n",
    "    \"Difference (meters)\",\n",
    "    \"Density\",\n",
    "    \"bathy_diff_stats_raw.png\",\n",
    "    xlims=(-1000, 1000),\n",
    ")\n",
    "print(f\"Max: {np.max(bathy_differences[~np.isnan(bathy_differences)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above we can see that the data resembles a normal distribution, but has some substantial outliers. So let's filter out the outliers and then plot the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the outliers\n",
    "bathy_differences_filtered = bathy_differences[np.abs(bathy_differences - bathy_mean_d) < 3 * bathy_std]\n",
    "plot_measurement_statistics(\n",
    "    bathy_differences_filtered,\n",
    "    \"Filtered Bathymetry Measurement Statistics\",\n",
    "    \"Difference (meters)\",\n",
    "    \"Density\",\n",
    "    \"bathy_diff_stats_filtered.png\",\n",
    "    xlims=(-250, 250),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the filtered data we can see that the data is much more normally distributed. We can now use this data to build a sensor model for the particle filter. This model is a simple zero-mean normal distribution with a standard deviation of 100 meters. The statistical mean of the data suggests that there might be a sensor bias, but do to the fact that bathymetric measurements provide a direct measurement of the vertical state, it is unclear if this is an error in the sensor or the state estimate.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental configuration\n",
    "\n",
    "There are a few questions that can be answered with this simulation:\n",
    "\n",
    "1. As a general long-term navigation aide - akin to a GPS system - how well does the particle filter perform? This can be answered by running the particle filter with a well-known initial state and tracking the drift error over time.\n",
    "2. How well does the particle filter perform in a short-term navigation scenario? In other words, given a de-localized INS, how well does the particle filter perform in recovering the true state? This scenario poses a more specific scenario where the platform needs a position fix from GPS but is unable to get one.\n",
    "\n",
    "I'll analyze both scenarios in this notebook by using two different initial states. The first state will be a well-known state (akin to a GPS fix) and the second will be a de-localized state (delocalized to a drift rate ~1 nmi).\n",
    "\n",
    "### Performance criteria\n",
    "\n",
    "The performance of the particle filter will be evaluated based on the following haversine distance error criteria:\n",
    "1. Overall weighted root-mean-square error of the entire particle cloud (a measure of the overall accuracy of the particle filter)\n",
    "2. Haversine error of the particle filter estimate (weighted average of all particles)\n",
    "3. Wasserstein distance between the particle filter estimate and the true state (a measure of the distribution of the particle filter estimate) where the covariance (variance along the diagonal) matrix is derived from the particle field.\n",
    "These criteria should ultimately be used to evaluate the performance with respect to two concepts: the overall accuracy of the particle filter estimate to recover truth, and with respect to the distance traveled by the platform.\n",
    "\n",
    "### Truth model\n",
    "\n",
    "The source data used in this experiment is fundamentally survey data (i.e. measurements that are time stamped and geolocated). I simulate IMU data in order to develop a trajectory, but this isn't specifically recorded IMU data. In order to generate a seperate effective truth, the simulated IMU data is integrated to generate an INS trajectory. This integrated trajectory is then in need of correction. For a truth comparison, I then use the GPS locations of the geophysical measurements to correct the IMU integration to develop a \"Truth\" trajectory. The same integrated trajectory is then used (through the NED velocities) to propagate the particle filter. The particle filter itself then is corrected using the bathymetric measurements. The error metric are then a comparison of the particle filter's estimate to the INS's \"truth\" estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pyins import transform\n",
    "from src.geophysical import gmt_toolbox as gmt\n",
    "\n",
    "#####\n",
    "from src.data_management import dbmgr\n",
    "from src.geophysical import particle_filter as pf\n",
    "from src.geophysical.gmt_toolbox import GeophysicalMap, MeasurementType, ReliefResolution\n",
    "import haversine as hs\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def save_simulation_results(\n",
    "    filename: str, pf_result: pd.DataFrame, trajectory: pd.DataFrame, trajectory_sd: pd.DataFrame\n",
    "):\n",
    "    # Create an HDF5 file\n",
    "    with h5py.File(f\"{filename}.h5\", \"w\") as f:\n",
    "        # Save the result dataframe\n",
    "        result_group = f.create_group(\"result\")\n",
    "        for column in pf_result.columns:\n",
    "            result_group.create_dataset(column, data=pf_result[column].values)\n",
    "\n",
    "        # Save the feedback.trajectory dataframe\n",
    "        trajectory_group = f.create_group(\"trajectory\")\n",
    "        for column in trajectory.columns:\n",
    "            trajectory_group.create_dataset(column, data=trajectory[column].values)\n",
    "\n",
    "        # Save the feedback.trajectory_sd dataframe\n",
    "        trajectory_sd_group = f.create_group(\"trajectory_sd\")\n",
    "        for column in trajectory_sd.columns:\n",
    "            trajectory_sd_group.create_dataset(column, data=trajectory_sd[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbmgr.DatabaseManager(\".db\")\n",
    "all_trajs = db.get_all_trajectories()\n",
    "pf_config = pf.ParticleFilterConfig.from_dict(\n",
    "    {\n",
    "        \"n\": 100,\n",
    "        \"cov\": [0.01, 0.01, 1, 0.1, 0.1, 0.1, 0, 0, 0, 0],\n",
    "        \"noise\": [0.0, 0.0, 0.0, 1.75, 1.75, 0.01, 0.001, 0.001, 0.001, 0.1],\n",
    "        \"input_config\": pf.ParticleFilterInputConfig.VELOCITY,\n",
    "        \"measurement_config\": [{\"name\": \"bathymetry\", \"std\": 100}],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_trajectories = all_trajs[all_trajs[\"depth\"]]\n",
    "bathy_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_trajectories.loc[bathy_trajectories[\"id\"] == 900][\"duration\"] <= 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bathy_trajectories = bathy_trajectories[bathy_trajectories[\"duration\"] >= 3600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bathy_trajectories.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get_trajectory(108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory for saving results\n",
    "output_dir = \"bathy_pf_results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the data\n",
    "for j in tqdm(bathy_trajectories[\"id\"]):\n",
    "    if bathy_trajectories.loc[bathy_trajectories.index[j], \"duration\"] < 3600:\n",
    "        print(f\"Trajectory {j} is too short: {all_trajs.iloc[j]['duration']}\")\n",
    "        continue\n",
    "    name = db.get_all_trajectories()[\"source\"][j] + \"_\" + str(j)\n",
    "\n",
    "    try:\n",
    "        truth = db.get_trajectory(j)\n",
    "    except:\n",
    "        print(f\"Failed to get trajectory {j}\")\n",
    "        continue\n",
    "    try:\n",
    "        _, feedback = pf.calculate_truth(truth)\n",
    "        trajectory = feedback.trajectory\n",
    "        ins_errors = transform.compute_state_difference(truth, trajectory)\n",
    "    except:\n",
    "        print(f\"Failed to calculate truth for trajectory {j}\")\n",
    "        continue\n",
    "    distance = truth[\"distance\"].to_numpy()\n",
    "\n",
    "    min_lat = truth.lat.min()\n",
    "    max_lat = truth.lat.max()\n",
    "    min_lon = truth.lon.min()\n",
    "    max_lon = truth.lon.max()\n",
    "\n",
    "    try:\n",
    "        bathy_map = GeophysicalMap(\n",
    "            MeasurementType.BATHYMETRY, ReliefResolution.FIFTEEN_SECONDS, min_lon, max_lon, min_lat, max_lat, 0.25\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Failed to get bathymetry map for trajectory {j}\")\n",
    "        continue\n",
    "    geomaps = {gmt.MeasurementType.BATHYMETRY: bathy_map}\n",
    "    try:\n",
    "        result = pf.run_particle_filter(truth, trajectory, geomaps, pf_config)\n",
    "    except:\n",
    "        print(f\"Failed to run particle filter for trajectory {j}\")\n",
    "        continue\n",
    "    try:\n",
    "        out_path = os.path.join(output_dir, f\"{name}\")\n",
    "        save_simulation_results(f\"{out_path}\", result, trajectory, feedback.trajectory_sd)\n",
    "    except:\n",
    "        print(f\"Failed to save results for trajectory {j}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(bathy_map.map_data.lon, bathy_map.map_data.lat, bathy_map.map_data, cmap=\"ocean\")\n",
    "plt.plot(truth.lon, truth.lat, \"k.\", label=\"Truth\")\n",
    "plt.plot(trajectory.lon, trajectory.lat, \"g.\", label=\"INS\")\n",
    "plt.plot(result.lon, result.lat, \"y.\", label=\"PF\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"2D trajectory\")\n",
    "# plt.xlim(min_lon, max_lon)\n",
    "# plt.ylim(min_lat, max_lat)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv = hs.haversine_vector(truth[[\"lat\", \"lon\"]].to_numpy(), result[[\"lat\", \"lon\"]].to_numpy(), hs.Unit.METERS)\n",
    "plt.plot(result.index, result[\"rms_error_2d\"], \"b\")\n",
    "plt.plot(result.index, hsv, \"g\")\n",
    "plt.plot(result.index, result[\"estimate_error\"], \"y\")\n",
    "plt.ylim(0, 3000)\n",
    "plt.axhline(1852)\n",
    "plt.axhline(452, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "error_summary = pd.Series(err, distance)\n",
    "summary[f\"{j}\"] = error_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(distance, estimate[:, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results as a function of distance traveled\n",
    "plt.plot(\n",
    "    distance / 1852,\n",
    "    100 * np.sqrt(ins_errors.north**2 + ins_errors.east**2) / distance,\n",
    "    \"g-\",\n",
    "    label=\"INS error\",\n",
    ")\n",
    "plt.plot(\n",
    "    distance / 1852,\n",
    "    100 * np.sqrt(integrator_error.north**2 + integrator_error.east**2) / distance,\n",
    "    \"r.\",\n",
    "    label=\"integrator error\",\n",
    ")\n",
    "plt.xlabel(\"Distance traveled (nmi)\")\n",
    "plt.ylabel(\"Position error (% of distance traveled)\")\n",
    "plt.title(\"Radial position error vs. distance traveled\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Post Processing\n",
    "\n",
    "Use this section to load and post process the results data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import haversine as hs\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import h5py\n",
    "from src.geophysical import gmt_toolbox as gmt\n",
    "from src.data_management.m77t import find_periods\n",
    "\n",
    "\n",
    "def load_simulation_results(filename: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        # Load the result dataframe\n",
    "        result_data = {column: f[\"result\"][column][:] for column in f[\"result\"]}\n",
    "        result = pd.DataFrame(result_data)\n",
    "\n",
    "        # Load the feedback.trajectory dataframe\n",
    "        trajectory_data = {column: f[\"trajectory\"][column][:] for column in f[\"trajectory\"]}\n",
    "        trajectory = pd.DataFrame(trajectory_data)\n",
    "\n",
    "        # Load the feedback.trajectory_sd dataframe\n",
    "        trajectory_sd_data = {column: f[\"trajectory_sd\"][column][:] for column in f[\"trajectory_sd\"]}\n",
    "        trajectory_sd = pd.DataFrame(trajectory_sd_data)\n",
    "\n",
    "    distance = hs.haversine_vector(\n",
    "        trajectory[[\"lat\", \"lon\"]].to_numpy(), trajectory[[\"lat\", \"lon\"]].shift(1).to_numpy(), hs.Unit.METERS\n",
    "    )\n",
    "    distance = np.cumsum(np.nan_to_num(distance, nan=0))\n",
    "\n",
    "    # time = trajectory.index.to_numpy()\n",
    "\n",
    "    result[\"distance\"] = distance\n",
    "    trajectory[\"distance\"] = distance\n",
    "\n",
    "    lat_lon_var = result[[\"lat_var\", \"lon_var\"]].to_numpy()\n",
    "    planar_variance = hs.haversine_vector(np.zeros_like(lat_lon_var), lat_lon_var, hs.Unit.METERS)\n",
    "    three_d_variance = np.sqrt(planar_variance**2 + result[\"alt_var\"].to_numpy() ** 2)\n",
    "\n",
    "    result[\"planar_variance\"] = planar_variance\n",
    "    result[\"three_d_variance\"] = three_d_variance\n",
    "\n",
    "    # trajectory_sd[\"distance\"] = distance\n",
    "\n",
    "    return result, trajectory, trajectory_sd\n",
    "\n",
    "\n",
    "def plot_estimate_error(\n",
    "    trajectory: pd.DataFrame,\n",
    "    result: pd.DataFrame,\n",
    "    drift_rate: np.ndarray,\n",
    "    map_resolution: float = 452,\n",
    "    figure_size: tuple[int, int] = (12, 6),\n",
    "    title_str: str = \"Particle Filter Estimate Error\",\n",
    "    recovery_offset: float = 0,\n",
    ") -> plt.Figure:\n",
    "    drift_rate += recovery_offset\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figure_size, gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "\n",
    "    duration = trajectory.index[-1] * 60\n",
    "    axes[0].plot(trajectory[\"distance\"] / 1000, drift_rate, \"m\", label=\"INS Drift Rate\")\n",
    "    axes[0].fill_between(\n",
    "        trajectory[\"distance\"] / 1000,\n",
    "        result[\"estimate_error\"] - result[\"planar_variance\"] / 2,\n",
    "        result[\"estimate_error\"] + result[\"planar_variance\"] / 2,\n",
    "        color=\"k\",\n",
    "        alpha=0.2,\n",
    "        label=\"PF planar certainty\",\n",
    "    )\n",
    "    axes[0].plot(trajectory[\"distance\"] / 1000, result[\"estimate_error\"], \"k\", label=\"Estimate error\")\n",
    "\n",
    "    axes[0].fill_between(\n",
    "        trajectory[\"distance\"] / 1000,\n",
    "        np.maximum(result[\"estimate_error\"] + result[\"planar_variance\"] / 2, map_resolution),\n",
    "        drift_rate,\n",
    "        where=result[\"estimate_error\"] + result[\"planar_variance\"] / 2 <= drift_rate,\n",
    "        color=\"magenta\",\n",
    "        alpha=0.3,\n",
    "        label=\"Estimate error less than drift\",\n",
    "    )\n",
    "\n",
    "    axes[0].fill_between(\n",
    "        trajectory[\"distance\"] / 1000,\n",
    "        result[\"estimate_error\"] + result[\"planar_variance\"] / 2,\n",
    "        map_resolution,\n",
    "        where=result[\"estimate_error\"] + result[\"planar_variance\"] / 2 <= map_resolution,\n",
    "        color=\"green\",\n",
    "        alpha=0.3,\n",
    "        label=\"Estimate Error less than map resolution\",\n",
    "    )\n",
    "    axes[0].axhline(452, color=\"g\", linestyle=\"--\", label=\"Map resolution\")\n",
    "    axes[0].set_xlabel(\"Distance traveled (km)\")\n",
    "    axes[0].set_ylabel(\"Error (meters)\")\n",
    "    axes[0].set_xlim(left=0, right=trajectory[\"distance\"].max() / 1000)\n",
    "    axes[0].set_ylim(bottom=0)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(f\"{title_str} | Duration: {duration/3600:0.2f} hours\")\n",
    "\n",
    "    # Compress the y-axis of the second plot\n",
    "    axes[1].set_aspect(aspect=\"auto\", adjustable=\"datalim\")\n",
    "    axes[1].plot(trajectory[\"distance\"] / 1000, result[\"planar_variance\"], \"k\", label=\"2D\")\n",
    "    axes[1].plot(trajectory[\"distance\"] / 1000, result[\"three_d_variance\"], \"b\", label=\"3D\")\n",
    "    axes[1].set_xlabel(\"Distance traveled (km)\")\n",
    "    axes[1].set_ylabel(\"Estimate Certainty (m)\")\n",
    "    axes[1].set_xlim(left=0, right=trajectory[\"distance\"].max() / 1000)\n",
    "    axes[1].set_ylim([0, 5000])\n",
    "    axes[1].legend()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def find_estimate_statistic(result: pd.DataFrame, threshold_mask: list | np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the periods where the estimate error is below a certain threshold\n",
    "    \"\"\"\n",
    "    position_recovery_periods = find_periods(threshold_mask)\n",
    "    recoveries = pd.DataFrame()\n",
    "    for inds in position_recovery_periods:\n",
    "        start = result.index[inds[0]] * 60\n",
    "        end = result.index[inds[1]] * 60\n",
    "        duration = end - start\n",
    "        distance = result[\"distance\"].iloc[inds[1]] - result[\"distance\"].iloc[inds[0]]\n",
    "        start_distance = result[\"distance\"].iloc[inds[0]]\n",
    "        end_distance = result[\"distance\"].iloc[inds[1]]\n",
    "        mean_error = result[\"estimate_error\"].iloc[inds[0] : inds[1]].mean()\n",
    "        median_error = result[\"estimate_error\"].iloc[inds[0] : inds[1]].median()\n",
    "        recovery = pd.DataFrame(\n",
    "            {\n",
    "                \"start\": [start],\n",
    "                \"end\": [end],\n",
    "                \"duration\": [duration],\n",
    "                \"mean_error\": [mean_error],\n",
    "                \"median_error\": [median_error],\n",
    "                \"distance\": [distance],\n",
    "                \"start_distance\": [start_distance],\n",
    "                \"end_distance\": [end_distance],\n",
    "                \"distance_traveled\": end_distance - start_distance,\n",
    "            }\n",
    "        )\n",
    "        if duration > 0:\n",
    "            recoveries = pd.concat([recoveries, recovery])\n",
    "\n",
    "    return recoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob(os.path.join(\"scripts\", \"bathy_pf_results_delocalized\", \"*.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "result, trajectory, trajectory_std = load_simulation_results(result_files[i])\n",
    "print(f\"file: {result_files[i]}\")\n",
    "fig = plot_estimate_error(\n",
    "    trajectory,\n",
    "    result,\n",
    "    drift_rate=trajectory.index.to_numpy() * 60 * 1852 / (24 * 3600),\n",
    "    figure_size=(12, 4),\n",
    "    title_str=\"De-localized Particle Filter Estimate Error\",\n",
    "    recovery_offset=1852,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_error_summary = pd.DataFrame()\n",
    "rms_2d_summary = pd.DataFrame()\n",
    "rms_3d_summary = pd.DataFrame()\n",
    "distances = pd.DataFrame()\n",
    "recoveries = pd.DataFrame()\n",
    "below_pixel = pd.DataFrame()\n",
    "certain_recoveries = pd.DataFrame()\n",
    "certain_below_pixel = pd.DataFrame()\n",
    "\n",
    "for root, dirs, files in os.walk(\"scripts/bathy_pf_results_delocalized\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".h5\"):\n",
    "            # filename = os.path.join(root, file).split('.')[0]\n",
    "            filename = file.split(\".\")[0]\n",
    "            result, trajectory, trajectory_std = load_simulation_results(os.path.join(root, f\"{filename}.h5\"))\n",
    "            drift_rate = trajectory.index.to_numpy() * 60 * 1852 / (24 * 3600)\n",
    "\n",
    "            estimate_error_plot = plot_estimate_error(\n",
    "                trajectory,\n",
    "                result,\n",
    "                drift_rate=drift_rate,\n",
    "                figure_size=(12, 4),\n",
    "                title_str=\"De-localized Particle Filter Estimate Error\",\n",
    "                recovery_offset=1852,\n",
    "            )\n",
    "            estimate_error_plot.savefig(os.path.join(root, f\"{filename}.png\"))\n",
    "            # estimate_error_plot.close()\n",
    "            plt.close(estimate_error_plot)\n",
    "            estimate_error = result[\"estimate_error\"].rename(filename)\n",
    "            estimate_error.index = result[\"distance\"]\n",
    "            estimate_error_summary = pd.merge(\n",
    "                estimate_error_summary, estimate_error, left_index=True, right_index=True, how=\"outer\"\n",
    "            )\n",
    "\n",
    "            recoveries = pd.concat(\n",
    "                [recoveries, find_estimate_statistic(result, result[\"estimate_error\"] >= drift_rate)]\n",
    "            )\n",
    "            below_pixel = pd.concat([below_pixel, find_estimate_statistic(result, result[\"estimate_error\"] >= 452)])\n",
    "\n",
    "            certain_recoveries = pd.concat(\n",
    "                [certain_recoveries, find_estimate_statistic(result, result[\"planar_variance\"] >= drift_rate)]\n",
    "            )\n",
    "            certain_below_pixel = pd.concat(\n",
    "                [below_pixel, find_estimate_statistic(result, result[\"planar_variance\"] >= 452)]\n",
    "            )\n",
    "\n",
    "    # rms_2d_summary = pd.concat([rms_2d_summary, result[\"rms_error_2d\"]], axis=1)\n",
    "    # rms_3d_summary = pd.concat([rms_3d_summary, result[\"rms_error_3d\"]], axis=1)\n",
    "    # distances = pd.concat([distances, result[\"distance\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certain_recoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_data(data: pd.DataFrame):\n",
    "    mean_start = timedelta(seconds=data.mean()[\"start\"])\n",
    "    mean_end = timedelta(seconds=data.mean()[\"end\"])\n",
    "    mean_duration = timedelta(seconds=data.mean()[\"duration\"])\n",
    "    print(f\"There were a total of {len(data)} position recoveries.\")\n",
    "    print(f\"Mean recovery start: {mean_start}\")\n",
    "    print(f\"Mean recovery end: {mean_end}\")\n",
    "    print(f\"Mean recovery duration: {mean_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary_data(recoveries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary_data(below_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary_data(certain_recoveries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certain_recoveries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this experiment is to test the particle filter's ability to estimate the true state, either by maintaining an originally accurate estimate, or by recovering from a de-localized state. We are simulating a marine-grade INS with a drift rate of 1 nm per 24 hours, but comparing and propagting this sytem to a true INS value with GPS measurements.\n",
    "\n",
    "Error metrics we are interested in:\n",
    "\n",
    "1. Raw particle filter estimate error in meters plus the standard deviation of the particle filter estimate (a measure of overall accuracy and confidence)\n",
    "2. Particle filter error normalized by the distance traveled by the platform (a measure of the relative error)\n",
    "3. Particle filter error normalized by drift rate (indicates how much of the error is due to the drift rate, and how much is due to the particle filter) which should demonstrate it's suitability as a long-term navigation aide for position feedback.\n",
    "4. Particle filter error (possibly normalized as above) as a percentage of map pixel resolution (15 arcseconds, ~452 meters).\n",
    "\n",
    "Key finding that we want to prove is that the particle filter can estimate the true state to within the resolution of a given map pixel. This is a key finding as it demonstrates the particle filter's ability to provide a position fix in the absence of GPS that is better than the resolution of the map. Alternatively, if we had an arbitrary map where each pixel had a completely unique signature, the measurement model would only be able to report the particle filter's estimate to within the resolution of the map. Subsequent movements and measurments could then be used to refine the estimate. Because geophysical maps and measurements are not unique, the ability of the particle filter to estimate the true state to within the resolution of the map is a key finding that indicates a performance level that approaches a theoretical limit.\n",
    "\n",
    "### General idea\n",
    "\n",
    "Consider a grid based map $m$ with entirely unique signatures $m = \\left[m_0 \\dots m_n\\right]$ where $m_i \\neq m_j$ for all $i \\neq j$. We have a means to measure that signature with perfect accuracy maing the measurement model $p(z | m) = 1$ for $z = m_i$, and zero elsewhere. $\\text{bel}(x_t) = \\text{bel}(x_{t-1}) * p(z | m)$. To derive a navigation fix we would then take the maximum value of $\\text{bel}(x_t)$ and use that as the navigation fix. However, without knowledge of motion, the state space is similarly limited to the grid cell size. With knowledge of motion, this measurement function becomes a lower limit to the amount of error the position estimate can be resolved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d = trajectory_std[[\"north\", \"east\"]]\n",
    "ins_certainty = np.sqrt(\n",
    "    hs.haversine_vector(two_d.to_numpy(), np.zeros(two_d.shape), hs.Unit.METERS) ** 2 + trajectory_std[\"down\"] ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Estimate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate accuracy w.r.t. distance traveled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    trajectory[\"distance\"] / 1000,\n",
    "    result[\"estimate_error\"].to_numpy() / trajectory[\"distance\"],\n",
    "    \"b\",\n",
    "    label=\"Relative to distance traveled\",\n",
    ")\n",
    "# plt.plot(trajectory[\"distance\"] / 1000, result[\"estimate_error\"].to_numpy() / drift_rate, 'r', label=\"Relative to INS drift rate\")\n",
    "# plt.fill_between(trajectory[\"distance\"] / 1000,\n",
    "#                  result[\"estimate_error\"] / trajectory[\"distance\"],\n",
    "#                  452 / trajectory[\"distance\"],\n",
    "#                  where=result[\"estimate_error\"] <= 452, color='green', alpha=0.3, label='Estimate Error <= 452')\n",
    "# plt.axhline(452, color='g', linestyle='--', label='Map Resolution (15 arcsec ~= 452m)')\n",
    "plt.xlabel(\"Distance traveled (km)\")\n",
    "plt.ylabel(\"Error (meters)\")\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.legend()\n",
    "plt.title(\"Particle Filter Estimate Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if all the tables in results_tables are present in summary[\"name\"] and if not capture the missing tables\n",
    "missing = []\n",
    "for table in results_tables:\n",
    "    if table not in summary[\"name\"].values:\n",
    "        missing.append(table)\n",
    "\n",
    "total = len(results_tables)\n",
    "num_recoveries = total - len(missing)\n",
    "print(\n",
    "    f\"There are {total} total trajectories. We were able to recover at least one position fix below drift error in {num_recoveries} ({num_recoveries / total :0.4f}) trajectories.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = summary.loc[summary[\"min error\"] <= 452]\n",
    "\n",
    "# check to see if the tables in pixel are present in summary[\"name\"] and if not capture the missing tables\n",
    "missing = []\n",
    "for table in results_tables:\n",
    "    if table not in pixel[\"name\"].values:\n",
    "        missing.append(table)\n",
    "below_pixel_fixes = total - len(missing)\n",
    "\n",
    "print(\n",
    "    f\"There are {len(pixel)} total below pixel resolution fixes. We were able to achieve at least one position estimate below drift error in {below_pixel_fixes} ({below_pixel_fixes/total :0.4f}) trajectories.\"\n",
    ")\n",
    "print(f\"mean duration: {pixel['duration'].mean()} and median duration: {pixel['duration'].median()}\")\n",
    "print(f\"mean error: {pixel['min error'].mean()} and median error: {pixel['min error'].median()}\")\n",
    "print(f\"minium duration: {pixel['duration'].min()} and maximum duration: {pixel['duration'].max()}\")\n",
    "print(f\"minimum error: {pixel['min error'].min()} and maximum error: {pixel['min error'].max()}\")\n",
    "print(f\"mean start: {pixel['start'].mean()} and median start: {pixel['start'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"min error\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"start\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"duration\").tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"average_error\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the line in summary that has the closest to the mean duration\n",
    "summary.loc[abs(summary[\"duration\"] - summary[\"duration\"].median()) <= timedelta(minutes=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[\"duration\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"On average we were able to recover a position fix with an mean duration of {summary['duration'].mean()}, median duration of {summary['duration'].median()} and a mean error of {summary['average_error'].mean()} and median error {summary['average_error'].median()}.\"\n",
    ")\n",
    "\n",
    "print(f\"Minimum duration {summary['duration'].min()} and maximum duration {summary['duration'].max()}.\")\n",
    "print(f\"Minimum error {summary['average_error'].min()} and maximum error {summary['average_error'].max()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = summary.loc[summary[\"num\"] == 0]\n",
    "# first.head()\n",
    "\n",
    "print(\n",
    "    f\"The first position recover occurs with a mean of {first['start'].mean()} and median {first['start'].median()} after the start of the trajectory.\"\n",
    ")\n",
    "print(\n",
    "    f\"with an mean duration of {first['duration'].mean()}, median duration of {first['duration'].median()} and a mean error of {first['average_error'].mean()} and median error {first['average_error'].median()}.\"\n",
    ")\n",
    "\n",
    "print(f\"Minimum duration {first['duration'].min()} and maximum duration {first['duration'].max()}.\")\n",
    "print(f\"Minimum error {first['average_error'].min()} and maximum error {first['average_error'].max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravity\n",
    "\n",
    "Recreate the above simulation and measurment model development this time with gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import src.process_dataset as pdset\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "gravity_tables = [table for table in tables if \"_G_\" in table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_gravity = np.array([])\n",
    "\n",
    "for table in gravity_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    gravity_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"gravity\", \"01m\", \"temp\")\n",
    "    d_gravity = np.hstack([d_gravity, data[\"GRAV_ANOM\"] - get_map_point(gravity_map, data.LON, data.LAT)])\n",
    "\n",
    "config[\"gravity_mean_d\"] = np.mean(d_gravity, where=~np.isnan(d_gravity))\n",
    "config[\"gravity_std\"] = np.std(d_gravity, where=~np.isnan(d_gravity))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "plt.hist(d_gravity, bins=100, density=True)\n",
    "plt.xlim([-50, 75])\n",
    "plt.xlabel(\"Gravity Difference (mGal)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Gravity Anomaly Difference\")\n",
    "plt.savefig(\".db/plots/gravity_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnetics\n",
    "\n",
    "Recreation with magnetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "mag_tables = [table for table in tables if \"_M_\" in table]\n",
    "\n",
    "df = pdset.table_to_df(\".db/parsed.db\", mag_tables[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_magnetics = np.array([])\n",
    "\n",
    "for table in mag_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    mag_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"magnetic\", \"02m\", \"temp\")\n",
    "    d_magnetics = np.hstack([d_magnetics, data[\"MAG_RES\"] - get_map_point(mag_map, data.LON, data.LAT)])\n",
    "\n",
    "config[\"magnetic_mean_d\"] = np.mean(d_magnetics, where=~np.isnan(d_magnetics))\n",
    "config[\"magnetic_std\"] = np.std(d_magnetics, where=~np.isnan(d_magnetics))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_magnetics, bins=100, density=True)\n",
    "plt.xlim([-500, 500])\n",
    "plt.xlabel(\"Magnetic Difference (nT)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Magnetic Residual Difference\")\n",
    "plt.savefig(\".db/plots/mag_diff.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
