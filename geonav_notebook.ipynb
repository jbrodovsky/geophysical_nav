{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bathymetric particle filter \n",
    "\n",
    "This notebook is a testing and development environment for the geophysical particle filter. As of 30 December 2024 the initial implementation will be a velocity-state based implementation that does not do the proposed full state mechanization. The full state mechanization will be implemented in the future under a ROS2 and Gazebo simulation environment that allows for more realistic (and reliable) IMU simulation.\n",
    "\n",
    "This notebook is meant for interactive testing and development and should not be used for large-scale simulations. The intended workflow is to use the notebook to test and develop the particle filter on a small subsample of test data, then build and install it in the local virtual environment for use in the full simulation.\n",
    "\n",
    "This full simulation should then be written in the `/scripts` folder with result data saved off there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters verification\n",
    "\n",
    "First we need to tune the particle filter propagation noise to be similar to that of a marine-grade inertial navigation system. A low-end marine-grade INS should have a drift of 1 nm per 24 hours. Our typical time interval is 60 seconds and the noise parameters for velocities states need to be tuned based on this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'propagate_ned' from 'geophysical_nav.geophysical.particle_filter' (/home/james/Code/geophysical_nav/src/geophysical_nav/geophysical/particle_filter.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeophysical_nav\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeophysical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparticle_filter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m propagate_ned, rmse\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_velocity_loop\u001b[39m(\n\u001b[32m      6\u001b[39m     u: np.ndarray, noise: np.ndarray, dt: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m60.0\u001b[39m, time: \u001b[38;5;28mint\u001b[39m = \u001b[32m24\u001b[39m * \u001b[32m60\u001b[39m\n\u001b[32m      7\u001b[39m ) -> \u001b[38;5;28mfloat\u001b[39m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'propagate_ned' from 'geophysical_nav.geophysical.particle_filter' (/home/james/Code/geophysical_nav/src/geophysical_nav/geophysical/particle_filter.py)"
     ]
    }
   ],
   "source": [
    "from geophysical_nav.geophysical.particle_filter import propagate_ned, rmse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_velocity_loop(\n",
    "    u: np.ndarray, noise: np.ndarray, dt: float = 60.0, time: int = 24 * 60\n",
    ") -> float:\n",
    "    P = np.asarray([[0, 0, 0, u[0], u[1], u[2]]])\n",
    "    T = P.copy()\n",
    "    t = 0\n",
    "    while t < time:\n",
    "        P = propagate_ned(\n",
    "            P, u, dt\n",
    "        )  # noise=np.diag(noise), noise_calibration_mode=True)\n",
    "        # Add zero-mean noise to the particles\n",
    "        P += np.random.multivariate_normal(np.zeros(6), np.diag(noise))\n",
    "        T = propagate_ned(\n",
    "            T, u, dt\n",
    "        )  # noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "        t += 1\n",
    "    error = rmse(P, T, include_altitude=True)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 24 * 60  # minutes\n",
    "vel_noise = 1.75  # m/s\n",
    "noise = np.array([0, 0, 0, vel_noise, vel_noise, 0.01])\n",
    "bound = 1852  # meters\n",
    "errors = []\n",
    "errors_dict = {}\n",
    "for i in range(1, 26):  # 1 to 25 nautical miles per hour\n",
    "    print(f\"Running for {i} knots\")\n",
    "    v = i * bound / 3600  # convert to m/s\n",
    "    for a in tqdm(range(1000)):\n",
    "        # Eastward\n",
    "        u = np.asarray([0.0, v, 0.0])\n",
    "        errors.append(run_velocity_loop(u, noise))\n",
    "        # Northward\n",
    "        u = np.asarray([v, 0, 0])\n",
    "        errors.append(run_velocity_loop(u, noise))\n",
    "        # Northeastward\n",
    "        u = np.array([1, 1, 0]) / np.linalg.norm([1, 1, 0])\n",
    "        u *= v\n",
    "        errors.append(run_velocity_loop(u, noise))\n",
    "    errors_dict[i] = errors.copy()\n",
    "    print(f\"Mean RMSE: {np.mean(errors): 0.2f} meters\")\n",
    "print(f\"Overall RMSE: {np.mean(errors)}\")\n",
    "# Plot the previous\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.violinplot(errors_dict.values(), showmeans=True, showmedians=True)\n",
    "plt.axhline(y=1852, color=\"k\", linestyle=\"--\", label=\"1 Nautical Mile\")\n",
    "plt.axhline(np.mean(errors), color=\"r\", linestyle=\"-\", label=\"Mean Error\")\n",
    "plt.xlabel(\"Velocity (knots)\")\n",
    "plt.ylabel(\"Error (meters)\")\n",
    "plt.title(\"Velocity Noise Tuning Error Distributions\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(\"velocity_noise_tuning.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalized noise model: $\\begin{bmatrix} 0 & 0 & 0 & 1.75 & 1.75 & 0.01 \\end{bmatrix}$\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop measurement model\n",
    "\n",
    "Next we need to develop the measurement value standard deviation. We'll first do some general examination of the data. Namely, investigating the sensor measurements to see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management import dbmgr\n",
    "\n",
    "db = dbmgr.DatabaseManager(\".db\")\n",
    "summary = db.get_all_trajectories()\n",
    "summary = summary.drop(summary.index[-1])\n",
    "bathy_trajectories = summary[summary[\"depth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.geophysical import gmt_toolbox as gmt\n",
    "from tqdm import tqdm\n",
    "\n",
    "bathy_differences = np.empty((0,))\n",
    "\n",
    "for id in tqdm(bathy_trajectories[\"id\"]):\n",
    "    data = db.get_trajectory(id)\n",
    "    min_lon = data[\"lon\"].min()\n",
    "    max_lon = data[\"lon\"].max()\n",
    "    min_lat = data[\"lat\"].min()\n",
    "    max_lat = data[\"lat\"].max()\n",
    "    # min_lon, min_lat, max_lon, max_lat = gmt.inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    try:\n",
    "        bathy_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.BATHYMETRY,\n",
    "            gmt.ReliefResolution.FIFTEEN_SECONDS,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.25,\n",
    "        )\n",
    "    except AssertionError:\n",
    "        if min_lat == max_lat:\n",
    "            min_lat -= 0.25\n",
    "            max_lat += 0.25\n",
    "        if min_lon == max_lon:\n",
    "            min_lon -= 0.25\n",
    "            max_lon += 0.25\n",
    "        bathy_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.BATHYMETRY,\n",
    "            gmt.ReliefResolution.FIFTEEN_SECONDS,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.1,\n",
    "        )\n",
    "    except:\n",
    "        print(\n",
    "            f\"Failed to get bathymetry map for {id}: {min_lon}, {max_lon}, {min_lat}, {max_lat}\"\n",
    "        )\n",
    "    # d_bathy = np.hstack([d_bathy, data[\"DEPTH\"] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "    bathy_differences = np.append(\n",
    "        bathy_differences,\n",
    "        data[\"depth\"] - (-bathy_map.get_map_point(data[\"lon\"], data[\"lat\"])),\n",
    "    )\n",
    "\n",
    "bathy_mean_d = np.mean(bathy_differences, where=~np.isnan(bathy_differences))\n",
    "bathy_std = np.std(bathy_differences, where=~np.isnan(bathy_differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def plot_measurement_statistics(\n",
    "    data: np.ndarray,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    filename: str,\n",
    "    bin_count: int = 1000,\n",
    "    xlims: tuple = None,\n",
    "):\n",
    "    Mean = np.mean(data, where=~np.isnan(data))\n",
    "    Std = np.std(data, where=~np.isnan(data))\n",
    "    Median = np.median(data[~np.isnan(data)])\n",
    "    Mode = mode(data[~np.isnan(data)]).mode\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(data, bins=bin_count, density=True, alpha=0.75)\n",
    "    plt.axvline(Mean, color=\"r\", linestyle=\"--\", label=f\"Mean={Mean:0.2f}\")\n",
    "    plt.axvline(Mean + Std, color=\"k\", linestyle=\"--\", label=f\"$\\pm\\sigma={Std:0.2f}$\")\n",
    "    plt.axvline(Mean - Std, color=\"k\", linestyle=\"--\")\n",
    "    plt.axvline(Median, color=\"g\", linestyle=\"--\", label=f\"Median={Median:0.2f}\")\n",
    "    plt.axvline(Mode, color=\"b\", linestyle=\"--\", label=f\"Mode={Mode:0.2f}\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xlims is not None:\n",
    "        plt.xlim(xlims)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above, let's plot the data and see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_measurement_statistics(\n",
    "    bathy_differences,\n",
    "    \"Raw Bathymetry Measurement Statistics\",\n",
    "    \"Difference (meters)\",\n",
    "    \"Density\",\n",
    "    \"bathy_diff_stats_raw.png\",\n",
    "    xlims=(-1000, 1000),\n",
    ")\n",
    "print(f\"Max: {np.max(bathy_differences[~np.isnan(bathy_differences)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above we can see that the data resembles a normal distribution, but has some substantial outliers. So let's filter out the outliers and then plot the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the outliers\n",
    "bathy_differences_filtered = bathy_differences[\n",
    "    np.abs(bathy_differences - bathy_mean_d) < 3 * bathy_std\n",
    "]\n",
    "plot_measurement_statistics(\n",
    "    bathy_differences_filtered,\n",
    "    \"Filtered Bathymetry Measurement Statistics\",\n",
    "    \"Difference (meters)\",\n",
    "    \"Density\",\n",
    "    \"bathy_diff_stats_filtered.png\",\n",
    "    xlims=(-250, 250),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the filtered data we can see that the data is much more normally distributed. We can now use this data to build a sensor model for the particle filter. This model is a simple zero-mean normal distribution with a standard deviation of 100 meters. The statistical mean of the data suggests that there might be a sensor bias, but do to the fact that bathymetric measurements provide a direct measurement of the vertical state, it is unclear if this is an error in the sensor or the state estimate.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental configuration\n",
    "\n",
    "There are a few questions that can be answered with this simulation:\n",
    "\n",
    "1. As a general long-term navigation aide - akin to a GPS system - how well does the particle filter perform? This can be answered by running the particle filter with a well-known initial state and tracking the drift error over time.\n",
    "2. How well does the particle filter perform in a short-term navigation scenario? In other words, given a de-localized INS, how well does the particle filter perform in recovering the true state? This scenario poses a more specific scenario where the platform needs a position fix from GPS but is unable to get one.\n",
    "\n",
    "I'll analyze both scenarios in this notebook by using two different initial states. The first state will be a well-known state (akin to a GPS fix) and the second will be a de-localized state (delocalized to a drift rate ~1 nmi).\n",
    "\n",
    "### Performance criteria\n",
    "\n",
    "The performance of the particle filter will be evaluated based on the following haversine distance error criteria:\n",
    "1. Overall weighted root-mean-square error of the entire particle cloud (a measure of the overall accuracy of the particle filter)\n",
    "2. Haversine error of the particle filter estimate (weighted average of all particles)\n",
    "3. Wasserstein distance between the particle filter estimate and the true state (a measure of the distribution of the particle filter estimate) where the covariance (variance along the diagonal) matrix is derived from the particle field.\n",
    "These criteria should ultimately be used to evaluate the performance with respect to two concepts: the overall accuracy of the particle filter estimate to recover truth, and with respect to the distance traveled by the platform.\n",
    "\n",
    "### Truth model\n",
    "\n",
    "The source data used in this experiment is fundamentally survey data (i.e. measurements that are time stamped and geolocated). I simulate IMU data in order to develop a trajectory, but this isn't specifically recorded IMU data. In order to generate a seperate effective truth, the simulated IMU data is integrated to generate an INS trajectory. This integrated trajectory is then in need of correction. For a truth comparison, I then use the GPS locations of the geophysical measurements to correct the IMU integration to develop a \"Truth\" trajectory. The same integrated trajectory is then used (through the NED velocities) to propagate the particle filter. The particle filter itself then is corrected using the bathymetric measurements. The error metric are then a comparison of the particle filter's estimate to the INS's \"truth\" estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pyins import transform\n",
    "from src.geophysical import gmt_toolbox as gmt\n",
    "\n",
    "#####\n",
    "from src.data_management import dbmgr\n",
    "from src.geophysical import particle_filter as pf\n",
    "from src.geophysical.gmt_toolbox import (\n",
    "    GeophysicalMap,\n",
    "    MeasurementType,\n",
    "    ReliefResolution,\n",
    ")\n",
    "import haversine as hs\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def save_simulation_results(\n",
    "    filename: str,\n",
    "    pf_result: pd.DataFrame,\n",
    "    trajectory: pd.DataFrame,\n",
    "    trajectory_sd: pd.DataFrame,\n",
    "):\n",
    "    # Create an HDF5 file\n",
    "    with h5py.File(f\"{filename}.h5\", \"w\") as f:\n",
    "        # Save the result dataframe\n",
    "        result_group = f.create_group(\"result\")\n",
    "        for column in pf_result.columns:\n",
    "            result_group.create_dataset(column, data=pf_result[column].values)\n",
    "\n",
    "        # Save the feedback.trajectory dataframe\n",
    "        trajectory_group = f.create_group(\"trajectory\")\n",
    "        for column in trajectory.columns:\n",
    "            trajectory_group.create_dataset(column, data=trajectory[column].values)\n",
    "\n",
    "        # Save the feedback.trajectory_sd dataframe\n",
    "        trajectory_sd_group = f.create_group(\"trajectory_sd\")\n",
    "        for column in trajectory_sd.columns:\n",
    "            trajectory_sd_group.create_dataset(\n",
    "                column, data=trajectory_sd[column].values\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbmgr.DatabaseManager(\".db\")\n",
    "all_trajs = db.get_all_trajectories()\n",
    "pf_config = pf.ParticleFilterConfig.from_dict(\n",
    "    {\n",
    "        \"n\": 100,\n",
    "        \"cov\": [0.01, 0.01, 1, 0.1, 0.1, 0.1, 0, 0, 0, 0],\n",
    "        \"noise\": [0.0, 0.0, 0.0, 1.75, 1.75, 0.01, 0.001, 0.001, 0.001, 0.1],\n",
    "        \"input_config\": pf.ParticleFilterInputConfig.VELOCITY,\n",
    "        \"measurement_config\": [{\"name\": \"bathymetry\", \"std\": 100}],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_trajectories = all_trajs[all_trajs[\"depth\"]]\n",
    "bathy_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_trajectories.loc[bathy_trajectories[\"id\"] == 900][\"duration\"] <= 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bathy_trajectories = bathy_trajectories[bathy_trajectories[\"duration\"] >= 3600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bathy_trajectories.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get_trajectory(108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory for saving results\n",
    "output_dir = \"bathy_pf_results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the data\n",
    "for j in tqdm(bathy_trajectories[\"id\"]):\n",
    "    if bathy_trajectories.loc[bathy_trajectories.index[j], \"duration\"] < 3600:\n",
    "        print(f\"Trajectory {j} is too short: {all_trajs.iloc[j]['duration']}\")\n",
    "        continue\n",
    "    name = db.get_all_trajectories()[\"source\"][j] + \"_\" + str(j)\n",
    "\n",
    "    try:\n",
    "        truth = db.get_trajectory(j)\n",
    "    except:\n",
    "        print(f\"Failed to get trajectory {j}\")\n",
    "        continue\n",
    "    try:\n",
    "        _, feedback = pf.calculate_truth(truth)\n",
    "        trajectory = feedback.trajectory\n",
    "        ins_errors = transform.compute_state_difference(truth, trajectory)\n",
    "    except:\n",
    "        print(f\"Failed to calculate truth for trajectory {j}\")\n",
    "        continue\n",
    "    distance = truth[\"distance\"].to_numpy()\n",
    "\n",
    "    min_lat = truth.lat.min()\n",
    "    max_lat = truth.lat.max()\n",
    "    min_lon = truth.lon.min()\n",
    "    max_lon = truth.lon.max()\n",
    "\n",
    "    try:\n",
    "        bathy_map = GeophysicalMap(\n",
    "            MeasurementType.BATHYMETRY,\n",
    "            ReliefResolution.FIFTEEN_SECONDS,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.25,\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Failed to get bathymetry map for trajectory {j}\")\n",
    "        continue\n",
    "    geomaps = {gmt.MeasurementType.BATHYMETRY: bathy_map}\n",
    "    try:\n",
    "        result = pf.run_particle_filter(truth, trajectory, geomaps, pf_config)\n",
    "    except:\n",
    "        print(f\"Failed to run particle filter for trajectory {j}\")\n",
    "        continue\n",
    "    try:\n",
    "        out_path = os.path.join(output_dir, f\"{name}\")\n",
    "        save_simulation_results(\n",
    "            f\"{out_path}\", result, trajectory, feedback.trajectory_sd\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Failed to save results for trajectory {j}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(\n",
    "    bathy_map.map_data.lon, bathy_map.map_data.lat, bathy_map.map_data, cmap=\"ocean\"\n",
    ")\n",
    "plt.plot(truth.lon, truth.lat, \"k.\", label=\"Truth\")\n",
    "plt.plot(trajectory.lon, trajectory.lat, \"g.\", label=\"INS\")\n",
    "plt.plot(result.lon, result.lat, \"y.\", label=\"PF\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"2D trajectory\")\n",
    "# plt.xlim(min_lon, max_lon)\n",
    "# plt.ylim(min_lat, max_lat)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv = hs.haversine_vector(\n",
    "    truth[[\"lat\", \"lon\"]].to_numpy(), result[[\"lat\", \"lon\"]].to_numpy(), hs.Unit.METERS\n",
    ")\n",
    "plt.plot(result.index, result[\"rms_error_2d\"], \"b\")\n",
    "plt.plot(result.index, hsv, \"g\")\n",
    "plt.plot(result.index, result[\"estimate_error\"], \"y\")\n",
    "plt.ylim(0, 3000)\n",
    "plt.axhline(1852)\n",
    "plt.axhline(452, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "error_summary = pd.Series(err, distance)\n",
    "summary[f\"{j}\"] = error_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(distance, estimate[:, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results as a function of distance traveled\n",
    "plt.plot(\n",
    "    distance / 1852,\n",
    "    100 * np.sqrt(ins_errors.north**2 + ins_errors.east**2) / distance,\n",
    "    \"g-\",\n",
    "    label=\"INS error\",\n",
    ")\n",
    "plt.plot(\n",
    "    distance / 1852,\n",
    "    100 * np.sqrt(integrator_error.north**2 + integrator_error.east**2) / distance,\n",
    "    \"r.\",\n",
    "    label=\"integrator error\",\n",
    ")\n",
    "plt.xlabel(\"Distance traveled (nmi)\")\n",
    "plt.ylabel(\"Position error (% of distance traveled)\")\n",
    "plt.title(\"Radial position error vs. distance traveled\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Post Processing\n",
    "\n",
    "Use this section to load and post process the results data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import haversine as hs\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import h5py\n",
    "from src.geophysical import gmt_toolbox as gmt\n",
    "from src.data_management.m77t import find_periods\n",
    "from scripts import open_loop as ol\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob(os.path.join(\"scripts\", \"bathy_pf_results_delocalized\", \"*.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol.summarize_results(\"scripts/bathy_pf_results\", \"delocalized\")\n",
    "ol.summarize_results(\"scripts/bathy_pf_results\", \"localized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this experiment is to test the particle filter's ability to estimate the true state, either by maintaining an originally accurate estimate, or by recovering from a de-localized state. We are simulating a marine-grade INS with a drift rate of 1 nm per 24 hours, but comparing and propagting this sytem to a true INS value with GPS measurements.\n",
    "\n",
    "Error metrics we are interested in:\n",
    "\n",
    "1. Raw particle filter estimate error in meters plus the standard deviation of the particle filter estimate (a measure of overall accuracy and confidence)\n",
    "2. Particle filter error normalized by the distance traveled by the platform (a measure of the relative error)\n",
    "3. Particle filter error normalized by drift rate (indicates how much of the error is due to the drift rate, and how much is due to the particle filter) which should demonstrate it's suitability as a long-term navigation aide for position feedback.\n",
    "4. Particle filter error (possibly normalized as above) as a percentage of map pixel resolution (15 arcseconds, ~452 meters).\n",
    "\n",
    "Key finding that we want to prove is that the particle filter can estimate the true state to within the resolution of a given map pixel. This is a key finding as it demonstrates the particle filter's ability to provide a position fix in the absence of GPS that is better than the resolution of the map. Alternatively, if we had an arbitrary map where each pixel had a completely unique signature, the measurement model would only be able to report the particle filter's estimate to within the resolution of the map. Subsequent movements and measurments could then be used to refine the estimate. Because geophysical maps and measurements are not unique, the ability of the particle filter to estimate the true state to within the resolution of the map is a key finding that indicates a performance level that approaches a theoretical limit.\n",
    "\n",
    "### General idea\n",
    "\n",
    "Consider a grid based map $m$ with entirely unique signatures $m = \\left[m_0 \\dots m_n\\right]$ where $m_i \\neq m_j$ for all $i \\neq j$. We have a means to measure that signature with perfect accuracy maing the measurement model $p(z | m) = 1$ for $z = m_i$, and zero elsewhere. $\\text{bel}(x_t) = \\text{bel}(x_{t-1}) * p(z | m)$. To derive a navigation fix we would then take the maximum value of $\\text{bel}(x_t)$ and use that as the navigation fix. However, without knowledge of motion, the state space is similarly limited to the grid cell size. With knowledge of motion, this measurement function becomes a lower limit to the amount of error the position estimate can be resolved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravity and Magnetics\n",
    "\n",
    "Recreate the above measurment model development this time with gravity freeair anomaly and magnetic anomaly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management import dbmgr\n",
    "\n",
    "db = dbmgr.DatabaseManager(\".db\")\n",
    "summary = db.get_all_trajectories()\n",
    "summary = summary.drop(summary.index[-1])\n",
    "grav_trajectories = summary[summary[\"freeair\"]]\n",
    "mag_trajectories = summary[summary[\"mag_res\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.geophysical import gmt_toolbox as gmt\n",
    "from tqdm import tqdm\n",
    "\n",
    "grav_differences = np.empty((0,))\n",
    "\n",
    "for id in tqdm(grav_trajectories[\"id\"]):\n",
    "    data = db.get_trajectory(id)\n",
    "    min_lon = data[\"lon\"].min()\n",
    "    max_lon = data[\"lon\"].max()\n",
    "    min_lat = data[\"lat\"].min()\n",
    "    max_lat = data[\"lat\"].max()\n",
    "    # min_lon, min_lat, max_lon, max_lat = gmt.inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    try:\n",
    "        gravity_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.GRAVITY,\n",
    "            gmt.GravityResolution.ONE_MINUTE,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.25,\n",
    "        )\n",
    "    except AssertionError:\n",
    "        if min_lat == max_lat:\n",
    "            min_lat -= 0.25\n",
    "            max_lat += 0.25\n",
    "        if min_lon == max_lon:\n",
    "            min_lon -= 0.25\n",
    "            max_lon += 0.25\n",
    "        gravity_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.GRAVITY,\n",
    "            gmt.GravityResolution.ONE_MINUTE,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.1,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Failed to get gravity map for {id}: {min_lon}, {max_lon}, {min_lat}, {max_lat}\"\n",
    "        )\n",
    "        print(e)\n",
    "    # d_bathy = np.hstack([d_bathy, data[\"DEPTH\"] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "    grav_differences = np.append(\n",
    "        grav_differences,\n",
    "        data[\"freeair\"] - (gravity_map.get_map_point(data[\"lon\"], data[\"lat\"])),\n",
    "    )\n",
    "\n",
    "gravity_mean_d = np.mean(grav_differences, where=~np.isnan(grav_differences))\n",
    "gravity_std = np.std(grav_differences, where=~np.isnan(grav_differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_measurement_statistics(\n",
    "    grav_differences,\n",
    "    \"Raw Gravity Measurement Statistics\",\n",
    "    \"Difference (mgal)\",\n",
    "    \"Density\",\n",
    "    \"gravity_diff_stats_raw.png\",\n",
    "    xlims=(-50, 100),\n",
    ")\n",
    "print(f\"Max: {np.max(grav_differences[~np.isnan(grav_differences)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_differences = np.empty((0,))\n",
    "\n",
    "for id in tqdm(grav_trajectories[\"id\"]):\n",
    "    data = db.get_trajectory(id)\n",
    "    min_lon = data[\"lon\"].min()\n",
    "    max_lon = data[\"lon\"].max()\n",
    "    min_lat = data[\"lat\"].min()\n",
    "    max_lat = data[\"lat\"].max()\n",
    "    # min_lon, min_lat, max_lon, max_lat = gmt.inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    try:\n",
    "        mag_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.MAGNETIC,\n",
    "            gmt.MagneticResolution.TWO_MINUTES,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.25,\n",
    "        )\n",
    "    except AssertionError:\n",
    "        if min_lat == max_lat:\n",
    "            min_lat -= 0.25\n",
    "            max_lat += 0.25\n",
    "        if min_lon == max_lon:\n",
    "            min_lon -= 0.25\n",
    "            max_lon += 0.25\n",
    "        mag_map = gmt.GeophysicalMap(\n",
    "            gmt.MeasurementType.MAGNETIC,\n",
    "            gmt.MagneticResolution.TWO_MINUTES,\n",
    "            min_lon,\n",
    "            max_lon,\n",
    "            min_lat,\n",
    "            max_lat,\n",
    "            0.1,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Failed to get magnetic map for {id}: {min_lon}, {max_lon}, {min_lat}, {max_lat}\"\n",
    "        )\n",
    "        print(e)\n",
    "    # d_bathy = np.hstack([d_bathy, data[\"DEPTH\"] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "    mag_differences = np.append(\n",
    "        mag_differences,\n",
    "        data[\"mag_res\"] - (mag_map.get_map_point(data[\"lon\"], data[\"lat\"])),\n",
    "    )\n",
    "\n",
    "mag_mean_d = np.mean(mag_differences, where=~np.isnan(mag_differences))\n",
    "mag_std = np.std(mag_differences, where=~np.isnan(mag_differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_measurement_statistics(\n",
    "    mag_differences,\n",
    "    \"Raw Magnetic Measurement Statistics\",\n",
    "    \"Difference (mgal)\",\n",
    "    \"Density\",\n",
    "    \"magnetics_diff_stats_raw.png\",\n",
    "    xlims=(-1000, 1000),\n",
    ")\n",
    "print(f\"Max: {np.max(mag_differences[~np.isnan(mag_differences)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gravity Mean: {gravity_mean_d}, Gravity Std: {gravity_std}\")\n",
    "print(f\"Magnetic Mean: {mag_mean_d}, Magnetic Std: {mag_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_data = {\n",
    "    \"gravity_differences\": grav_differences,\n",
    "    \"magnetic_differences\": mag_differences,\n",
    "}\n",
    "\n",
    "with h5py.File(\"anomaly_data.h5\", \"w\") as f:\n",
    "    for key in anomaly_data:\n",
    "        f.create_dataset(key, data=anomaly_data[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts import open_loop as ol\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization = \"delocalized\"\n",
    "offset = 1852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_error_summary_distance = pd.read_csv(\n",
    "    f\"scripts/bathy_pf_results/{initialization}/estimate_error_summary_distance.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "estimate_error_summary_time = pd.read_csv(\n",
    "    f\"scripts/bathy_pf_results/{initialization}/estimate_error_summary_time.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "time = estimate_error_summary_time.index.to_numpy()\n",
    "drift = time * (1852 / (24 * 3600)) + offset\n",
    "mean = estimate_error_summary_time.mean(\n",
    "    axis=1\n",
    ")  # .rolling(window=1000, min_periods=1).mean().to_numpy()\n",
    "median = estimate_error_summary_time.median(\n",
    "    axis=1\n",
    ")  # .rolling(window=1000, min_periods=1).median().to_numpy()\n",
    "std = estimate_error_summary_time.std(\n",
    "    axis=1\n",
    ")  # .rolling(window=100, min_periods=1).std().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "ax.plot(time / 3600, mean / drift, \"-b\")\n",
    "ax.fill_between(\n",
    "    time / 3600, (mean - std) / drift, (mean + std) / drift, color=\"b\", alpha=0.25\n",
    ")\n",
    "ax.set_xlim(0, 150)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlabel(\"Time (hours)\")\n",
    "ax.set_ylabel(\"Multiple of Drift\")\n",
    "ax.set_title(f\"{initialization} Particle Filter Mean Percent Error Over Time\".title())\n",
    "ax.legend([\"Mean\", \"Standard Deviation\"])\n",
    "plt.savefig(f\"{initialization}_pf_error_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "ax.plot(time / 3600, median / drift, \"-g\")\n",
    "ax.fill_between(\n",
    "    time / 3600, (median - std) / drift, (median + std) / drift, color=\"g\", alpha=0.25\n",
    ")\n",
    "ax.set_xlim(0, 150)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlabel(\"Time (hours)\")\n",
    "ax.set_ylabel(\"Multiple of Drift\")\n",
    "ax.set_title(f\"{initialization} Particle Filter Median Percent Error Over Time\".title())\n",
    "ax.legend([\"Median\", \"Standard Deviation\"])\n",
    "plt.savefig(f\"{initialization}_pf_median_error_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "ax.plot(time / 3600, mean / drift, \"-b\")\n",
    "ax.plot(time / 3600, median / drift, \"-g\")\n",
    "ax.set_xlim(0, 24)\n",
    "ax.set_ylim(0, 15)\n",
    "ax.set_xlabel(\"Time (hours)\")\n",
    "ax.set_ylabel(\"Multiple of Drift\")\n",
    "ax.set_title(\n",
    "    f\"{initialization} Particle Filter Percent Error Over Time (Zoomed)\".title()\n",
    ")\n",
    "ax.axhline(1, color=\"m\", linestyle=\"--\", label=\"drift\")\n",
    "ax.legend([\"Mean\", \"Median\", \"Drift\"])\n",
    "plt.savefig(f\"{initialization}_pf_error_time_zoomed.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = pd.read_csv(\n",
    "    f\"scripts/bathy_pf_results/{initialization}/below_pixel.csv\", index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnetics Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import open_loop_mag as olm\n",
    "from src.data_management import dbmgr\n",
    "from src.geophysical import gmt_toolbox as gmt\n",
    "from src.geophysical import particle_filter as pf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbmgr.DatabaseManager(\".db\")\n",
    "all_trajs = db.get_all_trajectories()\n",
    "mag_trajectories = all_trajs.loc[\n",
    "    (all_trajs[\"duration\"] >= 3600) & (all_trajs[\"mag_res\"])\n",
    "]\n",
    "truth = db.get_trajectory(75)\n",
    "_, feedback = pf.calculate_truth(truth)\n",
    "trajectory = feedback.trajectory\n",
    "min_lat = truth.lat.min()\n",
    "max_lat = truth.lat.max()\n",
    "min_lon = truth.lon.min()\n",
    "max_lon = truth.lon.max()\n",
    "mag_map = gmt.GeophysicalMap(\n",
    "    gmt.MeasurementType.MAGNETIC,\n",
    "    gmt.MagneticResolution.TWO_MINUTES,\n",
    "    min_lon,\n",
    "    max_lon,\n",
    "    min_lat,\n",
    "    max_lat,\n",
    "    0.25,\n",
    ")\n",
    "geomaps = {gmt.MeasurementType.MAGNETIC: mag_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_config_localized = pf.ParticleFilterConfig.from_dict(\n",
    "    {\n",
    "        \"n\": 10,\n",
    "        \"cov\": [15 / (1852 * 60), 15 / (1852 * 60), 1, 0.1, 0.1, 0.1, 0, 0, 0, 0],\n",
    "        \"noise\": [0.0, 0.0, 0.0, 1.75, 1.75, 0.01, 0.001, 0.001, 0.001, 0.1],\n",
    "        \"input_config\": pf.ParticleFilterInputConfig.VELOCITY,\n",
    "        \"measurement_config\": [{\"name\": \"magnetic\", \"std\": 194}],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_config_localized.get_base_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_config_localized.measurement_config[0].name == gmt.MeasurementType.MAGNETIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olm.pf_config_delocalized.measurement_config[0].name == gmt.MeasurementType.BATHYMETRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.GeophysicalMeasurement.from_dict({\"name\": \"magnetic\", \"std\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = pf.initialize_particle_filter(\n",
    "    truth.loc[truth.index[0], pf_config_localized.get_base_state()].to_numpy(),\n",
    "    pf_config_localized,\n",
    ")\n",
    "weights = np.ones((pf_config_localized.n,)) / pf_config_localized.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pf.update_anomaly(\n",
    "    particles,\n",
    "    geomaps[pf_config_localized.measurement_config[0].name],\n",
    "    truth.loc[truth.index[2], \"mag_res\"],\n",
    "    pf_config_localized.measurement_config[0].std,\n",
    "    particles[:, 9],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pf.run_particle_filter(truth, trajectory, geomaps, pf_config_localized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravity Anomaly Measurement Testing\n",
    "\n",
    "Let's try to break out the multiple gaussians in this data set.\n",
    "\n",
    "![](gravity_diff_stats_raw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_data = h5py.File(\"anomaly_data.h5\", \"r\")\n",
    "grav = np.array(anomaly_data[\"gravity_differences\"])\n",
    "grav = grav[~np.isnan(grav)]\n",
    "num_bins = 10000\n",
    "\n",
    "\n",
    "# plt.hist(grav, bins=num_bins, density=True, alpha=0.75)\n",
    "hist = np.histogram(grav, bins=num_bins, density=True)\n",
    "plt.bar(hist[1][:-1], hist[0], width=np.diff(hist[1])[0], alpha=0.75)\n",
    "plt.xlabel(\"Gravity Anomaly (mgal)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(-20, 60)\n",
    "plt.ylim(0, 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter differences to remove outliers over sixty or under -10\n",
    "grav_filtered = grav[(grav < 60) & (grav > -10)]\n",
    "grav_mix = GaussianMixture(n_components=5).fit(grav_filtered.reshape(-1, 1))\n",
    "means = grav_mix.means_.squeeze()\n",
    "sigmas = grav_mix.covariances_.squeeze()\n",
    "# hist = np.histogram(grav_filtered, bins=num_bins, density=True)\n",
    "grav_labels_hist = grav_mix.predict(hist[1][:-1].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widthes = np.diff(hist[1])[0]\n",
    "colors = [\"r\", \"g\", \"b\", \"c\", \"m\"]\n",
    "for i in range(5):\n",
    "    xvals = hist[1][:-1][grav_labels_hist == i]\n",
    "    yvals = hist[0][grav_labels_hist == i]\n",
    "    plt.bar(\n",
    "        xvals,\n",
    "        yvals,\n",
    "        width=widthes,\n",
    "        alpha=0.5,\n",
    "        color=colors[i],\n",
    "        label=f\"$\\mu$={means[i]:0.2f}, $\\sigma$={sigmas[i]:0.2f}\",\n",
    "    )\n",
    "    plt.axvline(means[i], linestyle=\"--\", color=colors[i], alpha=0.75)\n",
    "    # plt.axvline(means[i] + sigmas[i], linestyle=\"-.\", color=colors[i])\n",
    "    # plt.axvline(means[i] - sigmas[i], linestyle=\"-.\", color=colors[i])\n",
    "plt.xlim(-20, 60)\n",
    "plt.ylim(0, 0.1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Gravity Anomaly (mgal)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Gravity Anomaly Distribution with Gaussian Mixture Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4.46 + 4.64 + 4.8 + 5.83) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.geophysical import particle_filter as pf\n",
    "import numpy as np\n",
    "\n",
    "pf_config_delocalized = pf.ParticleFilterConfig.from_dict(\n",
    "    {\n",
    "        \"n\": 5,\n",
    "        \"cov\": [0.01, 0.01, 1.0, 0.10, 0.10, 0.10, 0.000, 0.000, 0, 0],\n",
    "        \"noise\": [0.00, 0.00, 0.0, 1.75, 1.75, 0.01, 0.001, 0.001, 0.001, 0.1],\n",
    "        \"input_config\": pf.ParticleFilterInputConfig.VELOCITY,\n",
    "        \"measurement_config\": [{\"name\": \"MAGNETIC\", \"std\": 5}],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_config_delocalized.cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_config_delocalized.cov.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = np.array([75, -40, 10, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = pf.initialize_particle_filter(initial_state, pf_config_delocalized, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles[:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
