{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Water Bathymetric Particle Filter Experiment\n",
    "\n",
    "This notebook runs the experiment testing the capabilities of the particle filter to conduct navigation using deep water bathymetry and for long duration.\n",
    "\n",
    "## Data set preparation\n",
    "\n",
    "First need to process the .m77t files in `source_data` into our database format. Additionally, we will create a 'truth' mechanization for the INS to serve as our ground truth reference comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management import m77t as tbx\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from numpy import float64, int64, sin, cos, tan, rad2deg, deg2rad, eye\n",
    "from matplotlib import pyplot as plt\n",
    "from pyins import strapdown, transform, measurements, filters, sim, earth\n",
    "from numpy.random import multivariate_normal as mvn\n",
    "from src.geophysical import gmt_toolbox as gmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygmt.datasets import load_earth_relief, load_earth_free_air_anomaly, load_earth_magnetic_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relief = gmt.GeophysicalMap(gmt.MeasurementType.RELIEF, gmt.ReliefResolution.ONE_MINUTE, -1, 1, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relief.get_map_point(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truth Mechanization\n",
    "\n",
    "The truth mechanization will be a simple INS mechanization from the simulated trajectory values. This will be used to compare the performance of the particle filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.join(\"/\", \"mnt\", \"c\", \"Users\", \"james\", \"OneDrive - Temple University\")\n",
    "datapath = os.path.join(home, \"Documents\", \"source_data\", \"marine_tracklines\")\n",
    "outpath = os.path.join(home, \"Documents\", \"source_data\", \"db\")\n",
    "\n",
    "datain = tbx.process_m77t_file(os.path.join('test', 'test_data.m77t'))\n",
    "traj = datain[0].copy()\n",
    "traj_ins = traj.copy()\n",
    "traj_ins.index = traj.index.to_series().diff().dt.total_seconds().fillna(0).cumsum()\n",
    "\n",
    "increments = strapdown.compute_increments_from_imu(traj_ins, 'rate')\n",
    "observations = measurements.Position(sim.generate_position_measurements(traj_ins[[\"lat\", \"lon\", \"alt\"]], 5.0), 5.0)\n",
    "init_pva = traj_ins.loc[traj_ins.index[0], [\"lat\", \"lon\", \"alt\", \"VN\", \"VE\", \"VD\", \"roll\", \"pitch\", \"heading\"]]\n",
    "distance = traj_ins['distance'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mechanization\n",
    "feedback = filters.run_feedback_filter(init_pva, 5, 2, 1, 1, increments, measurements=[observations], time_step=1.0, with_altitude=True)\n",
    "errors = transform.compute_state_difference(traj_ins, feedback.trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results as a function of distance traveled\n",
    "plt.plot(distance / 1852, 100 * np.sqrt(errors.north**2 + errors.east**2) / distance, label=\"radial\")\n",
    "plt.xlabel('Distance traveled (nmi)')\n",
    "plt.ylabel('Position error (% of distance traveled)')\n",
    "plt.title(\"Radial position error vs. distance traveled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter Mechanization\n",
    "\n",
    "The particle filter mechanization will be a simple INS mechanization from the simulated trajectory values. This will be used to compare the performance of the particle filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.geophysical import particle_filter as pf\n",
    "from src.geophysical import gmt_toolbox as gmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfconfig = pf.ParticleFilterConfig.from_dict({\"n\": 100, \n",
    "            \"cov\": np.arange(18), \n",
    "            \"noise\": np.arange(18), \n",
    "            \"measurement_config\": [\n",
    "                {\"name\": \"bathymetry\", \"std\": 15},\n",
    "                {\"name\": \"gravity\", \"std\": 1},\n",
    "                {\"name\": \"magnetic\", \"std\": 5}\n",
    "                ] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lon = traj[\"lon\"].min()\n",
    "max_lon = traj[\"lon\"].max()\n",
    "min_lat = traj[\"lat\"].min()\n",
    "max_lat = traj[\"lat\"].max()\n",
    "min_lon, min_lat, max_lon, max_lat = pf.inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geomap = gmt.get_map_section(min_lon, max_lon, min_lat, max_lat, 'relief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement = pf.GeophysicalMeasurement(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(measurement.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle filter propagation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import cross, array, float64, int64\n",
    "from numpy.typing import NDArray\n",
    "import numpy as np\n",
    "from pyins import earth, transform\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from numba import njit, jit\n",
    "from numpy.random import multivariate_normal as mvn\n",
    "\n",
    "EARTH_RATE = earth.RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_ned(particles: NDArray[float64 | int64], gyros:NDArray[float64 | int64], accels: NDArray[float64 | int64], dt: float64 | int64) -> NDArray:\n",
    "    \"\"\"\n",
    "    Propagate the particles according to the strapdown INS equations using only\n",
    "    NumPy operations and a more Pythonic syntax.\n",
    "    \"\"\"\n",
    "     # Getting some constants\n",
    "    Rn, Re, _ = earth.principal_radii(particles[:, 0], particles[:, 2])\n",
    "    gravity_vector = earth.gravity_n(particles[:, 0], particles[:, 2])\n",
    "    # Prior values\n",
    "    lat_ = deg2rad(particles[:, 0])\n",
    "    lon_ = deg2rad(particles[:, 1])\n",
    "    alt_ = particles[:, 2]\n",
    "    vn_ = particles[:, 3]\n",
    "    ve_ = particles[:, 4]\n",
    "    vd_ = particles[:, 5]\n",
    "    C_ = transform.mat_from_rph(deg2rad(particles[:, 6:9]))\n",
    "\n",
    "    # Attitude update\n",
    "    Omega_ies = array(\n",
    "        [earth.RATE * vector_to_skew_symmetric([cos(particle[0]), 0, -sin(particle[1])]) for particle in particles])\n",
    "    omegas = array([ ve_/(Re + alt_), \n",
    "                    -vn_/(Rn + alt_), \n",
    "                    -ve_*tan(lat_)/(Re + alt_)]).T\n",
    "    Omega_ens = array([vector_to_skew_symmetric(omega) for omega in omegas])\n",
    "    Omega_ibs = array([vector_to_skew_symmetric(gyros - particle[9:12]) for particle in particles])\n",
    "\n",
    "    C = C_ @ (eye(3) + Omega_ibs*dt) - (Omega_ies + Omega_ens) @ C_ * dt\n",
    "\n",
    "    # Specific force transformation\n",
    "    f = 0.5 * (C + C_) @ accels\n",
    "\n",
    "    # Velocity update\n",
    "    q = Omega_ens + 2*Omega_ies\n",
    "    velocity = particles[:, 3:6] + (f - particles[:, 9:12] + gravity_vector - array([q_ @ v for q_, v in zip(q, particles[:, 3:6])]))*dt\n",
    "\n",
    "    # Position Update\n",
    "    alt = particles[:, 2] - dt/2 * (particles[:, 6] + particles[:, 3:6][:, 2])\n",
    "    lat = rad2deg(lat_ + dt/2 * (vn_ / (Rn + lat_) + velocity[:, 0] / (Rn + alt)))\n",
    "    lon = rad2deg(lon_ + dt/2 * (ve_ / ((Re + alt_) * cos(lat_)) + velocity[:, 1] / ((Re + alt) * cos(lat))))\n",
    "    rph = rad2deg(transform.mat_to_rph(C))\n",
    "    # Gyro and accel bias update\n",
    "    # gyro_bias = particles[:, 9:12] + mvn([0, 0, 0], noise[9:12], n)\n",
    "    # accel_bias = particles[:, 12:15] + mvn([0, 0, 0], noise[12:15], n)\n",
    "    return np.column_stack([lat, lon, alt, velocity, rph])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization parameters\n",
    "n = 100\n",
    "dt = 60\n",
    "i = 10\n",
    "mu = traj.iloc[i][[\"lat\", \"lon\", \"alt\", \"VN\", \"VE\", \"VD\", \"roll\", \"pitch\", \"heading\"]].to_numpy()\n",
    "mu = np.hstack((mu, np.zeros(6)))\n",
    "noise = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001])\n",
    "particles = mvn(mu, np.eye(15), n)\n",
    "\n",
    "# Coning and sculling with conversions doesn't work (is unneeded) for the PF\n",
    "# thetas, dvs = coning_and_sculling_correction(current_gyros, previous_gyros, current_accels, previous_accels)\n",
    "\n",
    "# Getting starting values and inputs\n",
    "gyros = traj.iloc[i][[\"gyro_x\", \"gyro_y\", \"gyro_z\"]].to_numpy() # * dt\n",
    "accels = traj.iloc[i][[\"accel_x\", \"accel_y\", \"accel_z\"]].to_numpy()\n",
    "\n",
    "propagate_ned(particles, gyros, accels, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# ============ JIT TEST ========================================================\n",
    "# Compile jit function\n",
    "n = 5000000\n",
    "print(\"Running compilation pass...\")\n",
    "start_time = time.perf_counter()\n",
    "propagate_imu(particles, gyros, accels, dt, noise)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed (with compilation) = {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compiled jit function\n",
    "for n in [1000, 5000, 10000, 50000, 100000, 250000, 500000, 750000, 1000000]:\n",
    "    particles = mvn(mu, np.eye(15), n)\n",
    "    start_time = time.perf_counter()\n",
    "    propagate_imu(particles, gyros, accels, dt, noise)\n",
    "    end_time = time.perf_counter()\n",
    "    jitted = end_time - start_time\n",
    "    # print(f\"JITTED: Elapsed (n={n}) = {jitted}s\")\n",
    "\n",
    "    # Run compiled jit function\n",
    "    start_time = time.perf_counter()\n",
    "    propagate_ned(particles, gyros, accels, dt)\n",
    "    end_time = time.perf_counter()\n",
    "    pythonic = end_time - start_time\n",
    "    #print(f\"PYTHONIC: Elapsed (n={n}) = {pythonic}s\")\n",
    "\n",
    "    print(f\"JIT is {pythonic/jitted:0.4f} times faster than Pythonic for n={n}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros_like, isnan, nansum, ones_like\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "OVERFLOW = 500\n",
    "sigma = 1\n",
    "n = 5000000\n",
    "z_bar = np.random.normal(0, 1, n)\n",
    "z = np.random.normal(0, 1, n)\n",
    "start_time = time.perf_counter()\n",
    "dz = z - z_bar\n",
    "#w = zeros_like(dz)\n",
    "#inds = abs(dz) < OVERFLOW\n",
    "#w[inds] = norm(loc=0, scale=sigma).pdf(dz[inds])\n",
    "\n",
    "w = norm(loc=0, scale=sigma).pdf(dz)\n",
    "w[isnan(w)] = 1e-16\n",
    "if any(isnan(w)):\n",
    "    print(\"NAN elements found\")\n",
    "    w[isnan(w)] = 1e-16\n",
    "\n",
    "w_sum = nansum(w)\n",
    "try:\n",
    "    new_weights = w / w_sum\n",
    "except ZeroDivisionError:\n",
    "    new_weights = ones_like(w) / particles.shape[0]\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed time: {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan * np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.NAN == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "@jit\n",
    "def go_fast(a:int): # Function is compiled to machine code when called the first time\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "print(go_fast(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def print_buffer():\n",
    "    print(\"-----------------\")\n",
    "\n",
    "@njit\n",
    "def go_fast_n_times(a:NDArray, n:int):\n",
    "    a = array(a)\n",
    "    for i in range(n):\n",
    "        print(go_fast(a))\n",
    "        print_buffer()\n",
    "\n",
    "go_fast_n_times([1,2,3], 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after doing some research and experimentation I've found two things:\n",
    "\n",
    "1. Numba functions can only call other jitted functions, not Python functions. So if `foo` is a jitted function that calls `bar`, `bar` must also be a jitted function.\n",
    "2. Numba really only works well with the foreknowledge that the data is going to be a certain type (numpy array) and known size. Functions like `squeeze` and calls to the `ndarray` class are not supported.\n",
    "\n",
    "A couple of thoughts and options here:\n",
    "\n",
    "* Write public wrapper functions that do all the input validations and then call a private jitted function.\n",
    "* Make use of type annotations and documentation to make it clear what the expected input is. If it breaks that's on the user.\n",
    "\n",
    "So: for the particle filter implementation, I think I need to go with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal as mvn\n",
    "from numpy.random import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([0, 1, 2])\n",
    "sigma = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "particles = mvn(mu, np.diag(sigma), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu + normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles + mvn(np.zeros((3,)), np.diag(sigma), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters verification\n",
    "\n",
    "First we need to tune the particle filter propagation noise to be similar to that of a marine-grade inertial navigation system. A low-end marine-grade INS should have a drift of 1 nm per 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.particle_filter import rmse, propagate\n",
    "import numpy as np\n",
    "\n",
    "time = 24 * 60  # minutes\n",
    "noise = np.array([0, 2.6, 0])\n",
    "bound = 1852  # meters\n",
    "\n",
    "errors = []\n",
    "for v in range(1, 26):\n",
    "    P = np.asarray([[0, 0, 0, 0, v, 0]])\n",
    "    T = P.copy()\n",
    "    t = 0\n",
    "    for i in range(50000):\n",
    "        # Eastward\n",
    "        u = [0, v, 0]\n",
    "        while t < time:\n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "            t += 1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "        # Northward\n",
    "        u = [v, 0, 0]\n",
    "        while t < time:\n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "            t += 1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "        # Northeastward\n",
    "        u = np.array([1, 1, 0]) / np.linalg.norm([1, 1, 0])\n",
    "        u *= v\n",
    "        while t < time:\n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "            t += 1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "\n",
    "print(f\"RMSE: {np.mean(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if not os.path.exists(\".db/plots\"):\n",
    "    os.makedirs(\".db/plots\")\n",
    "\n",
    "plt.hist(errors, bins=15, density=True)\n",
    "plt.xlabel(\"RMSE (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"RMSE of Particle Filter\")\n",
    "plt.savefig(\".db/plots/propagation_tuning.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"velocity_noise\": [noise[1], noise[1], 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop measurement model\n",
    "\n",
    "Next we need to develop the measurement value standard deviation. We'll first do some general examination of the data. Namely, investigating the sensor measurements to see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    config = json.load(open(\"config.json\", \"r\"))\n",
    "\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "bathy_tables = [table for table in tables if \"_D_\" in table]\n",
    "\n",
    "d_bathy = np.array([])\n",
    "\n",
    "for table in bathy_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    bathy_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"relief\", \"15s\", \"temp\")\n",
    "    d_bathy = np.hstack([d_bathy, data[\"DEPTH\"] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "\n",
    "config[\"bathy_mean_d\"] = np.mean(d_bathy, where=~np.isnan(d_bathy))\n",
    "config[\"bathy_std\"] = np.std(d_bathy, where=~np.isnan(d_bathy))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_bathy, bins=200, density=True)\n",
    "plt.xlim([-300, 300])\n",
    "plt.xlabel(\"Depth Difference (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bathymetry Difference\")\n",
    "plt.savefig(\".db/plots/bathy_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate with velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.particle_filter import (\n",
    "    process_particle_filter,\n",
    "    populate_velocities,\n",
    "    plot_error,\n",
    "    plot_estimate,\n",
    "    summarize_results,\n",
    ")\n",
    "import json\n",
    "import src.process_dataset as pdset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "bathy_tables = [table for table in tables if \"_D_\" in table]\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "config[\"n\"] = 1000\n",
    "config[\"cov\"] = [\n",
    "    1 / 60,\n",
    "    1 / 60,\n",
    "    0,\n",
    "    config[\"velocity_noise\"][0],\n",
    "    config[\"velocity_noise\"][1],\n",
    "    0,\n",
    "]\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdset.table_to_df(\".db/parsed.db\", bathy_tables[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\".db/plots2/estimate/\"):\n",
    "    os.makedirs(\".db/plots2/estimate/\")\n",
    "\n",
    "if not os.path.exists(\".db/plots2/errors/\"):\n",
    "    os.makedirs(\".db/plots2/errors/\")\n",
    "\n",
    "summary = None\n",
    "annotations = {\"recovery\": 1852, \"res\": 1852 / 4}\n",
    "# for table in tqdm(bathy_tables):\n",
    "#     print(f\"Running {table}\")\n",
    "#     df = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "\n",
    "df = populate_velocities(df)\n",
    "results, geo_map = process_particle_filter(df, config)\n",
    "print(\"Run complete! Saving results...\")\n",
    "pdset.save_dataset(\n",
    "    [results],\n",
    "    [\"test\"],\n",
    "    output_location=\".db\",\n",
    "    output_format=\"db\",\n",
    "    dataset_name=\"results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results saved! Plotting...\")\n",
    "fig, ax = plot_estimate(geo_map, results)\n",
    "fig.savefig(f\".db/plots2/estimate/test_estimate.png\")\n",
    "plt.close(fig)\n",
    "fig, ax = plot_error(results, annotations=annotations)\n",
    "fig.savefig(f\".db/plots2/errors/test_error.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "for table in results_tables:\n",
    "    df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    run = summarize_results(df, 1852)\n",
    "    run[\"Name\"] = table\n",
    "    if summary is None:\n",
    "        summary = run  # .copy()\n",
    "    else:\n",
    "        summary = pd.concat([summary, run], ignore_index=True)\n",
    "    summary.to_csv(\".db/summary_recovery.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "for table in results_tables:\n",
    "    df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    run = summarize_results(df, 452)\n",
    "    run[\"Name\"] = table\n",
    "    if summary is None:\n",
    "        summary = run  # .copy()\n",
    "    else:\n",
    "        summary = pd.concat([summary, run], ignore_index=True)\n",
    "    summary.to_csv(\".db/summary_resolution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Post Processing\n",
    "\n",
    "Use this section to load and post process the results data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.process_dataset as pdset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.particle_filter import summarize_results\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "len(results_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in results_tables:\n",
    "    table_df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    summary = summarize_results(table, table_df, 1852)\n",
    "    summary.to_csv(\n",
    "        \".db/plots/summary.csv\",\n",
    "        mode=\"a\",\n",
    "        header=(not os.path.exists(\".db/plots/summary.csv\")),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(\n",
    "    \".db/plots/summary.csv\",\n",
    "    header=0,\n",
    "    dtype={\n",
    "        \"\": int,\n",
    "        \"name\": str,\n",
    "        \"start\": str,\n",
    "        \"stop\": str,\n",
    "        \"duration\": str,\n",
    "        \"average_error\": float,\n",
    "        \"max_error\": float,\n",
    "        \"min_error\": float,\n",
    "    },\n",
    ")\n",
    "summary[\"num\"] = summary[\"Unnamed: 0\"]\n",
    "summary = summary.drop(columns=[\"Unnamed: 0\"])\n",
    "# summary['start'] = pd.to_datetime(summary['start'], format=\"%Y-%m-%d %H:%M:%S%z\")\n",
    "# summary['end'] = pd.to_datetime(summary['end'], format=\"%Y-%m-%d %H:%M:%S%z\")\n",
    "summary[\"start\"] = pd.to_timedelta(summary[\"start\"])\n",
    "summary[\"end\"] = pd.to_timedelta(summary[\"end\"])\n",
    "summary[\"duration\"] = pd.to_timedelta(summary[\"duration\"])\n",
    "summary.head()\n",
    "\n",
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery = summary.loc[summary[\"min error\"] > 452]\n",
    "len(recovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if all the tables in results_tables are present in summary[\"name\"] and if not capture the missing tables\n",
    "missing = []\n",
    "for table in results_tables:\n",
    "    if table not in summary[\"name\"].values:\n",
    "        missing.append(table)\n",
    "\n",
    "total = len(results_tables)\n",
    "num_recoveries = total - len(missing)\n",
    "print(\n",
    "    f\"There are {total} total trajectories. We were able to recover at least one position fix below drift error in {num_recoveries} ({num_recoveries / total :0.4f}) trajectories.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = summary.loc[summary[\"min error\"] <= 452]\n",
    "\n",
    "# check to see if the tables in pixel are present in summary[\"name\"] and if not capture the missing tables\n",
    "missing = []\n",
    "for table in results_tables:\n",
    "    if table not in pixel[\"name\"].values:\n",
    "        missing.append(table)\n",
    "below_pixel_fixes = total - len(missing)\n",
    "\n",
    "print(\n",
    "    f\"There are {len(pixel)} total below pixel resolution fixes. We were able to achieve at least one position estimate below drift error in {below_pixel_fixes} ({below_pixel_fixes/total :0.4f}) trajectories.\"\n",
    ")\n",
    "print(f\"mean duration: {pixel['duration'].mean()} and median duration: {pixel['duration'].median()}\")\n",
    "print(f\"mean error: {pixel['min error'].mean()} and median error: {pixel['min error'].median()}\")\n",
    "print(f\"minium duration: {pixel['duration'].min()} and maximum duration: {pixel['duration'].max()}\")\n",
    "print(f\"minimum error: {pixel['min error'].min()} and maximum error: {pixel['min error'].max()}\")\n",
    "print(f\"mean start: {pixel['start'].mean()} and median start: {pixel['start'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"min error\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"start\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"duration\").tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"average_error\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the line in summary that has the closest to the mean duration\n",
    "summary.loc[abs(summary[\"duration\"] - summary[\"duration\"].median()) <= timedelta(minutes=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[\"duration\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"On average we were able to recover a position fix with an mean duration of {summary['duration'].mean()}, median duration of {summary['duration'].median()} and a mean error of {summary['average_error'].mean()} and median error {summary['average_error'].median()}.\"\n",
    ")\n",
    "\n",
    "print(f\"Minimum duration {summary['duration'].min()} and maximum duration {summary['duration'].max()}.\")\n",
    "print(f\"Minimum error {summary['average_error'].min()} and maximum error {summary['average_error'].max()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = summary.loc[summary[\"num\"] == 0]\n",
    "# first.head()\n",
    "\n",
    "print(\n",
    "    f\"The first position recover occurs with a mean of {first['start'].mean()} and median {first['start'].median()} after the start of the trajectory.\"\n",
    ")\n",
    "print(\n",
    "    f\"with an mean duration of {first['duration'].mean()}, median duration of {first['duration'].median()} and a mean error of {first['average_error'].mean()} and median error {first['average_error'].median()}.\"\n",
    ")\n",
    "\n",
    "print(f\"Minimum duration {first['duration'].min()} and maximum duration {first['duration'].max()}.\")\n",
    "print(f\"Minimum error {first['average_error'].min()} and maximum error {first['average_error'].max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravity\n",
    "\n",
    "Recreate the above simulation and measurment model development this time with gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import src.process_dataset as pdset\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "gravity_tables = [table for table in tables if \"_G_\" in table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_gravity = np.array([])\n",
    "\n",
    "for table in gravity_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    gravity_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"gravity\", \"01m\", \"temp\")\n",
    "    d_gravity = np.hstack([d_gravity, data[\"GRAV_ANOM\"] - get_map_point(gravity_map, data.LON, data.LAT)])\n",
    "\n",
    "config[\"gravity_mean_d\"] = np.mean(d_gravity, where=~np.isnan(d_gravity))\n",
    "config[\"gravity_std\"] = np.std(d_gravity, where=~np.isnan(d_gravity))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "plt.hist(d_gravity, bins=100, density=True)\n",
    "plt.xlim([-50, 75])\n",
    "plt.xlabel(\"Gravity Difference (mGal)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Gravity Anomaly Difference\")\n",
    "plt.savefig(\".db/plots/gravity_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnetics\n",
    "\n",
    "Recreation with magnetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "mag_tables = [table for table in tables if \"_M_\" in table]\n",
    "\n",
    "df = pdset.table_to_df(\".db/parsed.db\", mag_tables[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_magnetics = np.array([])\n",
    "\n",
    "for table in mag_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    mag_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"magnetic\", \"02m\", \"temp\")\n",
    "    d_magnetics = np.hstack([d_magnetics, data[\"MAG_RES\"] - get_map_point(mag_map, data.LON, data.LAT)])\n",
    "\n",
    "config[\"magnetic_mean_d\"] = np.mean(d_magnetics, where=~np.isnan(d_magnetics))\n",
    "config[\"magnetic_std\"] = np.std(d_magnetics, where=~np.isnan(d_magnetics))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_magnetics, bins=100, density=True)\n",
    "plt.xlim([-500, 500])\n",
    "plt.xlabel(\"Magnetic Difference (nT)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Magnetic Residual Difference\")\n",
    "plt.savefig(\".db/plots/mag_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"\":\n",
    "    print(\"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = [str(i) for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST = \"\".join(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"A\" in \"AaBbCc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx, names = tbx.parse_trackline_from_file(\"./test/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not []:\n",
    "    print(\"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"G\" in \"DMG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"78123006.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "        \"./test/test_data.m77t\",\n",
    "        header=0,\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "        dtype={\n",
    "            \"LAT\": float,\n",
    "            \"LON\": float,\n",
    "            \"CORR_DEPTH\": float,\n",
    "            \"MAG_TOT\": float,\n",
    "            \"MAG_RES\": float,\n",
    "            \"GRA_OBS\": float,\n",
    "            \"FREEAIR\": float,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"AB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_manager import m77t as tbx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tbx.read_m77t(\"./source_data/78123006.m77t\")\n",
    "data = tbx.m77t_to_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pd.io.json.build_table_schema(data)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "def get_engine(server, database, username, password):\n",
    "    \"\"\"Creates and returns an SQLAlchemy engine for SQL Server.\"\"\"\n",
    "    connection_string = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, Column, Integer, String, MetaData, select\n",
    "from sqlalchemy.dialects.mssql import NVARCHAR, FLOAT\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define your table schema as reflected from the database or defined by your application\n",
    "collections = Table('Collections', metadata,\n",
    "                    Column('CollectionID', Integer, primary_key=True),\n",
    "                    Column('CollectionName', NVARCHAR(255), nullable=False),\n",
    "                    Column('Description', NVARCHAR('max'))\n",
    "                    )\n",
    "\n",
    "def insert_collection(engine, collection_name, description=\"\"):\n",
    "    \"\"\"Insert a new collection using SQLAlchemy and return its CollectionID.\"\"\"\n",
    "    insert_stmt = collections.insert().values(CollectionName=collection_name, Description=description)\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(insert_stmt)\n",
    "        conn.commit()\n",
    "        collection_id = result.inserted_primary_key[0]  # Get the primary key of the inserted record\n",
    "    return collection_id\n",
    "\n",
    "def upload_data(df, collection_id, engine):\n",
    "    \"\"\"Uploads data from a DataFrame to the RawData table, linking it to the specified CollectionID.\"\"\"\n",
    "    df['CollectionID'] = collection_id  # Add CollectionID to the DataFrame\n",
    "    df.to_sql(name='RawData', con=engine, if_exists='append', index=False)\n",
    "\n",
    "def query_data_by_collection(collection_id, engine):\n",
    "    \"\"\"Queries data for a specific collection and returns it as a DataFrame.\"\"\"\n",
    "    query = f\"SELECT * FROM RawData WHERE CollectionID = {collection_id}\"\n",
    "    return pd.read_sql_query(query, con=engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine(\"localhost\", \"geonavdb\", \"sa\", \"2016SpringGardenStreet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_id = insert_collection(engine, \"test_collection\", \"This is a test collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geonav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
