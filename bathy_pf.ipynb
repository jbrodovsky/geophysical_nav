{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Water Bathymetric Particle Filter Experiment\n",
    "\n",
    "This notebook runs the experiment testing the capabilities of the particle filter to conduct navigation using deep water bathymetry and for long duration.\n",
    "\n",
    "## Data set preparation\n",
    "\n",
    "First need to process the .m77t files in `source_data` into our database format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.geophysical import m77t_toolbox as tbx\n",
    "from src.geophysical import db_tools as db\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, n = tbx.process_mgd77('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = tbx.split_and_validate_dataset(d[0], data_types=[[\"DG\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess tracklines into sql/df format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and see if the .db directory exists\n",
    "if not os.path.exists(\".db\"):\n",
    "    os.mkdir(\".db\")\n",
    "\n",
    "# Check to see if the database exists\n",
    "if not os.path.exists(\".db/tracklines.db\"):\n",
    "    tables = []\n",
    "else:\n",
    "    tables = db.get_tables(\".db/tracklines.db\")\n",
    "\n",
    "source_data_location = \"./source_data/\"\n",
    "# walk through the .m77t files in the source_data directory\n",
    "for root, dirs, files in os.walk(source_data_location):\n",
    "    for file in files:\n",
    "        if file.endswith(\".m77t\"):\n",
    "            # check to see if the file has already been processed\n",
    "            filename = os.path.splitext(file)[0]\n",
    "            if filename not in tables:\n",
    "                print(\"Processing file: \" + file)\n",
    "                data = pd.read_csv(os.path.join(root, file), delimiter=\"\\t\", header=0)\n",
    "                data = tbx.m77t_to_df(data)\n",
    "                # data.to_sql(\n",
    "                #    filename, sqlite3.connect(\".db/tracklines.db\"), if_exists=\"replace\"\n",
    "                # )\n",
    "                tbx.save_dataset(\n",
    "                    [data],\n",
    "                    [filename],\n",
    "                    output_location=\".db\",\n",
    "                    output_format=\"db\",\n",
    "                    dataset_name=\"tracklines\",\n",
    "                )\n",
    "            else:\n",
    "                print(\"Skipping file: \" + file + \" (already processed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the raw data into tracklines of continuous data collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 10  # minutes\n",
    "max_delta_t = 2  # minutes between points\n",
    "min_duration = 60  # minutes, minimum duration for a useful trackline\n",
    "\n",
    "data, names = pdset.parse_tracklines_from_db(\n",
    "    \".db/tracklines.db\",\n",
    "    # max_time,\n",
    "    # max_delta_t,\n",
    "    # min_duration,\n",
    "    data_types=[\n",
    "        \"bathy\",\n",
    "        \"mag\",\n",
    "        \"grav\",\n",
    "        [\"bathy\", \"mag\"],\n",
    "        [\"bathy\", \"grav\"],\n",
    "        [\"grav\", \"mag\"],\n",
    "        [\"bathy\", \"grav\", \"mag\"],\n",
    "    ],\n",
    ")\n",
    "# Save the parsed data to the database\n",
    "pdset.save_dataset(data, names, output_location=\".db\", output_format=\"db\", dataset_name=\"parsed\")\n",
    "# summary = pdset.get_parsed_data_summary(data, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters verification\n",
    "\n",
    "First we need to tune the particle filter propagation noise to be similar to that of a marine-grade inertial navigation system. A low-end marine-grade INS should have a drift of 1 nm per 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.particle_filter import rmse, propagate\n",
    "import numpy as np\n",
    "\n",
    "time = 24 * 60  # minutes\n",
    "noise = np.array([0, 2.6, 0])\n",
    "bound = 1852  # meters\n",
    "\n",
    "errors = []\n",
    "for v in range(1, 26):\n",
    "    P = np.asarray([[0, 0, 0, 0, v, 0]])\n",
    "    T = P.copy()\n",
    "    t = 0\n",
    "    for i in range(50000):\n",
    "        # Eastward\n",
    "        u = [0, v, 0]\n",
    "        while t < time:\n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "            t += 1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "        # Northward\n",
    "        u = [v, 0, 0]\n",
    "        while t < time:\n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "            t += 1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "        # Northeastward\n",
    "        u = np.array([1, 1, 0]) / np.linalg.norm([1, 1, 0])\n",
    "        u *= v\n",
    "        while t < time:\n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0, 0, 0]), noise_calibration_mode=False)\n",
    "            t += 1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "\n",
    "print(f\"RMSE: {np.mean(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if not os.path.exists(\".db/plots\"):\n",
    "    os.makedirs(\".db/plots\")\n",
    "\n",
    "plt.hist(errors, bins=15, density=True)\n",
    "plt.xlabel(\"RMSE (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"RMSE of Particle Filter\")\n",
    "plt.savefig(\".db/plots/propagation_tuning.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"velocity_noise\": [noise[1], noise[1], 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop measurement model\n",
    "\n",
    "Next we need to develop the measurement value standard deviation. We'll first do some general examination of the data. Namely, investigating the sensor measurements to see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    config = json.load(open(\"config.json\", \"r\"))\n",
    "\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "bathy_tables = [table for table in tables if \"_D_\" in table]\n",
    "\n",
    "d_bathy = np.array([])\n",
    "\n",
    "for table in bathy_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    bathy_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"relief\", \"15s\", \"temp\")\n",
    "    d_bathy = np.hstack([d_bathy, data[\"DEPTH\"] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "\n",
    "config[\"bathy_mean_d\"] = np.mean(d_bathy, where=~np.isnan(d_bathy))\n",
    "config[\"bathy_std\"] = np.std(d_bathy, where=~np.isnan(d_bathy))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_bathy, bins=200, density=True)\n",
    "plt.xlim([-300, 300])\n",
    "plt.xlabel(\"Depth Difference (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bathymetry Difference\")\n",
    "plt.savefig(\".db/plots/bathy_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate with velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.particle_filter import (\n",
    "    process_particle_filter,\n",
    "    populate_velocities,\n",
    "    plot_error,\n",
    "    plot_estimate,\n",
    "    summarize_results,\n",
    ")\n",
    "import json\n",
    "import src.process_dataset as pdset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "bathy_tables = [table for table in tables if \"_D_\" in table]\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "config[\"n\"] = 1000\n",
    "config[\"cov\"] = [\n",
    "    1 / 60,\n",
    "    1 / 60,\n",
    "    0,\n",
    "    config[\"velocity_noise\"][0],\n",
    "    config[\"velocity_noise\"][1],\n",
    "    0,\n",
    "]\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdset.table_to_df(\".db/parsed.db\", bathy_tables[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\".db/plots2/estimate/\"):\n",
    "    os.makedirs(\".db/plots2/estimate/\")\n",
    "\n",
    "if not os.path.exists(\".db/plots2/errors/\"):\n",
    "    os.makedirs(\".db/plots2/errors/\")\n",
    "\n",
    "summary = None\n",
    "annotations = {\"recovery\": 1852, \"res\": 1852 / 4}\n",
    "# for table in tqdm(bathy_tables):\n",
    "#     print(f\"Running {table}\")\n",
    "#     df = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "\n",
    "df = populate_velocities(df)\n",
    "results, geo_map = process_particle_filter(df, config)\n",
    "print(\"Run complete! Saving results...\")\n",
    "pdset.save_dataset(\n",
    "    [results],\n",
    "    [\"test\"],\n",
    "    output_location=\".db\",\n",
    "    output_format=\"db\",\n",
    "    dataset_name=\"results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results saved! Plotting...\")\n",
    "fig, ax = plot_estimate(geo_map, results)\n",
    "fig.savefig(f\".db/plots2/estimate/test_estimate.png\")\n",
    "plt.close(fig)\n",
    "fig, ax = plot_error(results, annotations=annotations)\n",
    "fig.savefig(f\".db/plots2/errors/test_error.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "for table in results_tables:\n",
    "    df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    run = summarize_results(df, 1852)\n",
    "    run[\"Name\"] = table\n",
    "    if summary is None:\n",
    "        summary = run  # .copy()\n",
    "    else:\n",
    "        summary = pd.concat([summary, run], ignore_index=True)\n",
    "    summary.to_csv(\".db/summary_recovery.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "for table in results_tables:\n",
    "    df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    run = summarize_results(df, 452)\n",
    "    run[\"Name\"] = table\n",
    "    if summary is None:\n",
    "        summary = run  # .copy()\n",
    "    else:\n",
    "        summary = pd.concat([summary, run], ignore_index=True)\n",
    "    summary.to_csv(\".db/summary_resolution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Post Processing\n",
    "\n",
    "Use this section to load and post process the results data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.process_dataset as pdset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.particle_filter import summarize_results\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "len(results_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in results_tables:\n",
    "    table_df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    summary = summarize_results(table, table_df, 1852)\n",
    "    summary.to_csv(\n",
    "        \".db/plots/summary.csv\",\n",
    "        mode=\"a\",\n",
    "        header=(not os.path.exists(\".db/plots/summary.csv\")),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(\n",
    "    \".db/plots/summary.csv\",\n",
    "    header=0,\n",
    "    dtype={\n",
    "        \"\": int,\n",
    "        \"name\": str,\n",
    "        \"start\": str,\n",
    "        \"stop\": str,\n",
    "        \"duration\": str,\n",
    "        \"average_error\": float,\n",
    "        \"max_error\": float,\n",
    "        \"min_error\": float,\n",
    "    },\n",
    ")\n",
    "summary[\"num\"] = summary[\"Unnamed: 0\"]\n",
    "summary = summary.drop(columns=[\"Unnamed: 0\"])\n",
    "# summary['start'] = pd.to_datetime(summary['start'], format=\"%Y-%m-%d %H:%M:%S%z\")\n",
    "# summary['end'] = pd.to_datetime(summary['end'], format=\"%Y-%m-%d %H:%M:%S%z\")\n",
    "summary[\"start\"] = pd.to_timedelta(summary[\"start\"])\n",
    "summary[\"end\"] = pd.to_timedelta(summary[\"end\"])\n",
    "summary[\"duration\"] = pd.to_timedelta(summary[\"duration\"])\n",
    "summary.head()\n",
    "\n",
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery = summary.loc[summary[\"min error\"] > 452]\n",
    "len(recovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if all the tables in results_tables are present in summary[\"name\"] and if not capture the missing tables\n",
    "missing = []\n",
    "for table in results_tables:\n",
    "    if table not in summary[\"name\"].values:\n",
    "        missing.append(table)\n",
    "\n",
    "total = len(results_tables)\n",
    "num_recoveries = total - len(missing)\n",
    "print(\n",
    "    f\"There are {total} total trajectories. We were able to recover at least one position fix below drift error in {num_recoveries} ({num_recoveries / total :0.4f}) trajectories.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = summary.loc[summary[\"min error\"] <= 452]\n",
    "\n",
    "# check to see if the tables in pixel are present in summary[\"name\"] and if not capture the missing tables\n",
    "missing = []\n",
    "for table in results_tables:\n",
    "    if table not in pixel[\"name\"].values:\n",
    "        missing.append(table)\n",
    "below_pixel_fixes = total - len(missing)\n",
    "\n",
    "print(\n",
    "    f\"There are {len(pixel)} total below pixel resolution fixes. We were able to achieve at least one position estimate below drift error in {below_pixel_fixes} ({below_pixel_fixes/total :0.4f}) trajectories.\"\n",
    ")\n",
    "print(f\"mean duration: {pixel['duration'].mean()} and median duration: {pixel['duration'].median()}\")\n",
    "print(f\"mean error: {pixel['min error'].mean()} and median error: {pixel['min error'].median()}\")\n",
    "print(f\"minium duration: {pixel['duration'].min()} and maximum duration: {pixel['duration'].max()}\")\n",
    "print(f\"minimum error: {pixel['min error'].min()} and maximum error: {pixel['min error'].max()}\")\n",
    "print(f\"mean start: {pixel['start'].mean()} and median start: {pixel['start'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"min error\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"start\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"duration\").tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by=\"average_error\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the line in summary that has the closest to the mean duration\n",
    "summary.loc[abs(summary[\"duration\"] - summary[\"duration\"].median()) <= timedelta(minutes=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[\"duration\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"On average we were able to recover a position fix with an mean duration of {summary['duration'].mean()}, median duration of {summary['duration'].median()} and a mean error of {summary['average_error'].mean()} and median error {summary['average_error'].median()}.\"\n",
    ")\n",
    "\n",
    "print(f\"Minimum duration {summary['duration'].min()} and maximum duration {summary['duration'].max()}.\")\n",
    "print(f\"Minimum error {summary['average_error'].min()} and maximum error {summary['average_error'].max()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = summary.loc[summary[\"num\"] == 0]\n",
    "# first.head()\n",
    "\n",
    "print(\n",
    "    f\"The first position recover occurs with a mean of {first['start'].mean()} and median {first['start'].median()} after the start of the trajectory.\"\n",
    ")\n",
    "print(\n",
    "    f\"with an mean duration of {first['duration'].mean()}, median duration of {first['duration'].median()} and a mean error of {first['average_error'].mean()} and median error {first['average_error'].median()}.\"\n",
    ")\n",
    "\n",
    "print(f\"Minimum duration {first['duration'].min()} and maximum duration {first['duration'].max()}.\")\n",
    "print(f\"Minimum error {first['average_error'].min()} and maximum error {first['average_error'].max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravity\n",
    "\n",
    "Recreate the above simulation and measurment model development this time with gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import src.process_dataset as pdset\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "gravity_tables = [table for table in tables if \"_G_\" in table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_gravity = np.array([])\n",
    "\n",
    "for table in gravity_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    gravity_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"gravity\", \"01m\", \"temp\")\n",
    "    d_gravity = np.hstack([d_gravity, data[\"GRAV_ANOM\"] - get_map_point(gravity_map, data.LON, data.LAT)])\n",
    "\n",
    "config[\"gravity_mean_d\"] = np.mean(d_gravity, where=~np.isnan(d_gravity))\n",
    "config[\"gravity_std\"] = np.std(d_gravity, where=~np.isnan(d_gravity))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "plt.hist(d_gravity, bins=100, density=True)\n",
    "plt.xlim([-50, 75])\n",
    "plt.xlabel(\"Gravity Difference (mGal)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Gravity Anomaly Difference\")\n",
    "plt.savefig(\".db/plots/gravity_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnetics\n",
    "\n",
    "Recreation with magnetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "mag_tables = [table for table in tables if \"_M_\" in table]\n",
    "\n",
    "df = pdset.table_to_df(\".db/parsed.db\", mag_tables[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_magnetics = np.array([])\n",
    "\n",
    "for table in mag_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    mag_map = get_map_section(min_lon, max_lon, min_lat, max_lat, \"magnetic\", \"02m\", \"temp\")\n",
    "    d_magnetics = np.hstack([d_magnetics, data[\"MAG_RES\"] - get_map_point(mag_map, data.LON, data.LAT)])\n",
    "\n",
    "config[\"magnetic_mean_d\"] = np.mean(d_magnetics, where=~np.isnan(d_magnetics))\n",
    "config[\"magnetic_std\"] = np.std(d_magnetics, where=~np.isnan(d_magnetics))\n",
    "\n",
    "if os.path.exists(\"config.json\"):\n",
    "    # delete the file\n",
    "    os.remove(\"config.json\")\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_magnetics, bins=100, density=True)\n",
    "plt.xlim([-500, 500])\n",
    "plt.xlabel(\"Magnetic Difference (nT)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Magnetic Residual Difference\")\n",
    "plt.savefig(\".db/plots/mag_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"\":\n",
    "    print(\"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = [str(i) for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST = \"\".join(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"A\" in \"AaBbCc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx, names = tbx.parse_trackline_from_file(\"./test/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_data_0',\n",
       " 'test_data_1',\n",
       " 'test_data_2',\n",
       " 'test_data_3',\n",
       " 'test_data_4',\n",
       " 'test_data_5',\n",
       " 'test_data_6',\n",
       " 'test_data_7',\n",
       " 'test_data_8',\n",
       " 'test_data_9',\n",
       " 'test_data_10',\n",
       " 'test_data_11',\n",
       " 'test_data_12',\n",
       " 'test_data_13',\n",
       " 'test_data_14',\n",
       " 'test_data_15',\n",
       " 'test_data_16',\n",
       " 'test_data_17',\n",
       " 'test_data_18',\n",
       " 'test_data_19',\n",
       " 'test_data_20',\n",
       " 'test_data_21',\n",
       " 'test_data_22',\n",
       " 'test_data_23',\n",
       " 'test_data_24',\n",
       " 'test_data_25',\n",
       " 'test_data_26',\n",
       " 'test_data_27',\n",
       " 'test_data_28',\n",
       " 'test_data_29',\n",
       " 'test_data_30',\n",
       " 'test_data_31',\n",
       " 'test_data_32',\n",
       " 'test_data_33',\n",
       " 'test_data_34',\n",
       " 'test_data_35',\n",
       " 'test_data_36',\n",
       " 'test_data_37',\n",
       " 'test_data_38',\n",
       " 'test_data_39',\n",
       " 'test_data_40',\n",
       " 'test_data_41',\n",
       " 'test_data_42',\n",
       " 'test_data_43',\n",
       " 'test_data_44',\n",
       " 'test_data_45',\n",
       " 'test_data_46',\n",
       " 'test_data_47',\n",
       " 'test_data_48',\n",
       " 'test_data_49',\n",
       " 'test_data_50',\n",
       " 'test_data_51',\n",
       " 'test_data_52',\n",
       " 'test_data_53',\n",
       " 'test_data_54',\n",
       " 'test_data_55',\n",
       " 'test_data_56',\n",
       " 'test_data_57',\n",
       " 'test_data_58',\n",
       " 'test_data_59',\n",
       " 'test_data_60',\n",
       " 'test_data_61',\n",
       " 'test_data_62',\n",
       " 'test_data_63',\n",
       " 'test_data_64',\n",
       " 'test_data_65',\n",
       " 'test_data_66',\n",
       " 'test_data_67',\n",
       " 'test_data_68',\n",
       " 'test_data_69',\n",
       " 'test_data_70',\n",
       " 'test_data_71',\n",
       " 'test_data_72',\n",
       " 'test_data_73',\n",
       " 'test_data_74',\n",
       " 'test_data_75']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPTY\n"
     ]
    }
   ],
   "source": [
    "if not []:\n",
    "    print(\"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"G\" in \"DMG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12204/4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SURVEY_ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>BAT_TTIME</th>\n",
       "      <th>CORR_DEPTH</th>\n",
       "      <th>BAT_CPCO</th>\n",
       "      <th>BAT_TYPCO</th>\n",
       "      <th>MAG_TOT</th>\n",
       "      <th>MAG_RES</th>\n",
       "      <th>FREEAIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979-05-19 02:30:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>16.22972</td>\n",
       "      <td>-99.81965</td>\n",
       "      <td>7.117</td>\n",
       "      <td>5354.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>41081.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979-05-19 02:45:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>16.18094</td>\n",
       "      <td>-99.81402</td>\n",
       "      <td>7.044</td>\n",
       "      <td>5298.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40930.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-123.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979-05-19 03:00:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>16.13214</td>\n",
       "      <td>-99.80842</td>\n",
       "      <td>6.460</td>\n",
       "      <td>4851.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40871.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>-114.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979-05-19 03:30:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>16.09372</td>\n",
       "      <td>-99.72320</td>\n",
       "      <td>6.227</td>\n",
       "      <td>4673.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40874.0</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>-103.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979-05-19 11:30:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>15.49778</td>\n",
       "      <td>-98.23145</td>\n",
       "      <td>6.157</td>\n",
       "      <td>4619.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40776.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-97.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1979-06-14 09:30:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>0.37312</td>\n",
       "      <td>-90.02423</td>\n",
       "      <td>2.047</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32170.0</td>\n",
       "      <td>-249.0</td>\n",
       "      <td>-2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1979-06-14 10:00:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>0.29111</td>\n",
       "      <td>-90.02782</td>\n",
       "      <td>0.982</td>\n",
       "      <td>736.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31783.0</td>\n",
       "      <td>-599.0</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1979-06-14 11:00:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>-90.03213</td>\n",
       "      <td>2.069</td>\n",
       "      <td>1543.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1979-06-14 12:00:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>-0.05231</td>\n",
       "      <td>-90.03878</td>\n",
       "      <td>1.099</td>\n",
       "      <td>823.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32236.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1979-06-14 14:00:00+00:00</td>\n",
       "      <td>78123006</td>\n",
       "      <td>-0.42106</td>\n",
       "      <td>-90.04931</td>\n",
       "      <td>1.300</td>\n",
       "      <td>972.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32270.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>-32.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unnamed: 0  SURVEY_ID       LAT       LON  BAT_TTIME  \\\n",
       "0    1979-05-19 02:30:00+00:00   78123006  16.22972 -99.81965      7.117   \n",
       "1    1979-05-19 02:45:00+00:00   78123006  16.18094 -99.81402      7.044   \n",
       "2    1979-05-19 03:00:00+00:00   78123006  16.13214 -99.80842      6.460   \n",
       "3    1979-05-19 03:30:00+00:00   78123006  16.09372 -99.72320      6.227   \n",
       "4    1979-05-19 11:30:00+00:00   78123006  15.49778 -98.23145      6.157   \n",
       "..                         ...        ...       ...       ...        ...   \n",
       "169  1979-06-14 09:30:00+00:00   78123006   0.37312 -90.02423      2.047   \n",
       "170  1979-06-14 10:00:00+00:00   78123006   0.29111 -90.02782      0.982   \n",
       "171  1979-06-14 11:00:00+00:00   78123006   0.12539 -90.03213      2.069   \n",
       "172  1979-06-14 12:00:00+00:00   78123006  -0.05231 -90.03878      1.099   \n",
       "173  1979-06-14 14:00:00+00:00   78123006  -0.42106 -90.04931      1.300   \n",
       "\n",
       "     CORR_DEPTH  BAT_CPCO  BAT_TYPCO  MAG_TOT  MAG_RES  FREEAIR  \n",
       "0        5354.0      43.0          1  41081.0     69.0   -128.3  \n",
       "1        5298.0      43.0          1  40930.0    -50.0   -123.3  \n",
       "2        4851.0      43.0          1  40871.0    -77.0   -114.3  \n",
       "3        4673.0      43.0          1  40874.0    -57.0   -103.7  \n",
       "4        4619.0      43.0          1  40776.0     85.0    -97.4  \n",
       "..          ...       ...        ...      ...      ...      ...  \n",
       "169      1527.0      41.0          1  32170.0   -249.0     -2.7  \n",
       "170       736.0      41.0          1  31783.0   -599.0     37.9  \n",
       "171      1543.0      41.0          1  32310.0      1.0    -21.8  \n",
       "172       823.0      41.0          1  32236.0      5.0     11.5  \n",
       "173       972.0      41.0          1  32270.0    199.0    -32.3  \n",
       "\n",
       "[174 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"78123006.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "Could not parse SQLAlchemy URL from string 'sql:http://127.0.0.1:10002/devstoreaccount1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Connect to azurite database\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43msa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql:http://127.0.0.1:10002/devstoreaccount1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sqlalchemy/util/deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    275\u001b[0m         _warn_with_version(\n\u001b[1;32m    276\u001b[0m             messages[m],\n\u001b[1;32m    277\u001b[0m             versions[m],\n\u001b[1;32m    278\u001b[0m             version_warnings[m],\n\u001b[1;32m    279\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    280\u001b[0m         )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/create.py:548\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty_in_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# create url.URL object\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43m_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m u, plugins, kwargs \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_instantiate_plugins(kwargs)\n\u001b[1;32m    552\u001b[0m entrypoint \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_get_entrypoint()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/url.py:838\u001b[0m, in \u001b[0;36mmake_url\u001b[0;34m(name_or_url)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given a string, produce a new URL instance.\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mThe format of the URL generally follows `RFC-1738\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m \n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, URL) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    840\u001b[0m     name_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sqla_is_testing_if_this_is_a_mock_object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m ):\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mArgumentError(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string or URL object, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_or_url\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/url.py:904\u001b[0m, in \u001b[0;36m_parse_url\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m URL\u001b[38;5;241m.\u001b[39mcreate(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponents)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mArgumentError(\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse SQLAlchemy URL from string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name\n\u001b[1;32m    906\u001b[0m     )\n",
      "\u001b[0;31mArgumentError\u001b[0m: Could not parse SQLAlchemy URL from string 'sql:http://127.0.0.1:10002/devstoreaccount1'"
     ]
    }
   ],
   "source": [
    "# Connect to azurite database\n",
    "engine = sa.create_engine(\"sql:http://127.0.0.1:10002/devstoreaccount1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
