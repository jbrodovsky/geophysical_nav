{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Water Bathymetric Particle Filter Experiment\n",
    "\n",
    "This notebook runs the experiment testing the capabilities of the particle filter to conduct navigation using deep water bathymetry and for long duration.\n",
    "\n",
    "## Data set preparation\n",
    "\n",
    "First need to process the .m77t files in `source_data` into our database format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import process_dataset as pdset\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess tracklines into sql/df format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and see if the .db directory exists\n",
    "if not os.path.exists(\".db\"):\n",
    "    os.mkdir(\".db\")\n",
    "    \n",
    "# Check to see if the database exists\n",
    "if not os.path.exists('.db/tracklines.db'):\n",
    "    tables = []\n",
    "else:\n",
    "    tables = pdset.get_tables('.db/tracklines.db')\n",
    "\n",
    "source_data_location = \"./source_data/\"\n",
    "# walk through the .m77t files in the source_data directory\n",
    "for root, dirs, files in os.walk(source_data_location):\n",
    "    for file in files:\n",
    "        if file.endswith(\".m77t\"):\n",
    "            # check to see if the file has already been processed\n",
    "            filename = os.path.splitext(file)[0]\n",
    "            if filename not in tables:\n",
    "                print(\"Processing file: \" + file)\n",
    "                data = pd.read_csv(os.path.join(root, file), delimiter=\"\\t\", header=0)\n",
    "                data = pdset.m77t_to_df(data)\n",
    "                # data.to_sql(\n",
    "                #    filename, sqlite3.connect(\".db/tracklines.db\"), if_exists=\"replace\"\n",
    "                # )\n",
    "                pdset.save_dataset(\n",
    "                    [data],\n",
    "                    [filename],\n",
    "                    output_location=\".db\",\n",
    "                    output_format=\"db\",\n",
    "                    dataset_name=\"tracklines\",\n",
    "                )\n",
    "            else:\n",
    "                print(\"Skipping file: \" + file + \" (already processed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the raw data into tracklines of continuous data collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 10 # minutes\n",
    "max_delta_t = 2 # minutes between points\n",
    "min_duration = 60 # minutes, minimum duration for a useful trackline\n",
    "\n",
    "data, names = pdset.parse_tracklines_from_db(\n",
    "    \".db/tracklines.db\",\n",
    "    #max_time,\n",
    "    #max_delta_t,\n",
    "    #min_duration,\n",
    "    data_types=[\n",
    "        \"bathy\",\n",
    "        \"mag\",\n",
    "        \"grav\",\n",
    "        [\"bathy\", \"mag\"],\n",
    "        [\"bathy\", \"grav\"],\n",
    "        [\"grav\", \"mag\"],\n",
    "        [\"bathy\", \"grav\", \"mag\"],\n",
    "    ],\n",
    ")\n",
    "# Save the parsed data to the database\n",
    "pdset.save_dataset(\n",
    "    data, names, output_location=\".db\", output_format=\"db\", dataset_name=\"parsed\"\n",
    ")\n",
    "#summary = pdset.get_parsed_data_summary(data, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters verification\n",
    "\n",
    "First we need to tune the particle filter propagation noise to be similar to that of a marine-grade inertial navigation system. A low-end marine-grade INS should have a drift of 1 nm per 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.particle_filter import rmse, propagate\n",
    "import numpy as np\n",
    "\n",
    "time = 24*60 # minutes\n",
    "noise = np.array([0, 2.6, 0])\n",
    "bound = 1852 # meters\n",
    "\n",
    "errors = []\n",
    "for v in range(1, 26):\n",
    "    P = np.asarray([[0,0,0,0,v,0]])\n",
    "    T = P.copy()\n",
    "    t = 0\n",
    "    for i in range(50000):\n",
    "        # Eastward\n",
    "        u = [0,v,0]\n",
    "        while t < time:    \n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0,0,0]), noise_calibration_mode=False)\n",
    "            t+=1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "        # Northward\n",
    "        u = [v,0,0]\n",
    "        while t < time:    \n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0,0,0]), noise_calibration_mode=False)\n",
    "            t+=1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "        # Northeastward \n",
    "        u = np.array([1,1,0]) / np.linalg.norm([1,1,0])\n",
    "        u *= v\n",
    "        while t < time:    \n",
    "            P = propagate(P, u, noise=np.diag(noise), noise_calibration_mode=True)\n",
    "            T = propagate(T, u, noise=np.diag([0,0,0]), noise_calibration_mode=False)\n",
    "            t+=1\n",
    "        errors.append(rmse(P, T[0, :2]))\n",
    "\n",
    "print(f\"RMSE: {np.mean(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if not os.path.exists(\".db/plots\"):\n",
    "    os.makedirs(\".db/plots\")\n",
    "\n",
    "plt.hist(errors, bins=25, density=True)\n",
    "plt.xlabel(\"RMSE (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"RMSE of Particle Filter\")\n",
    "plt.savefig(\".db/plots/propagation_tuning.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"velocity_noise\": [noise[1], noise[1], 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to develop the measurement value standard deviation. We'll first do some general examination of the data. Namely, investigating the sensor measurements to see if we can build a reasonable sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gmt_tool import inflate_bounds, get_map_section, get_map_point\n",
    "import numpy as np\n",
    "\n",
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "bathy_tables = [table for table in tables if \"_D_\" in table]\n",
    "\n",
    "d_bathy = np.array([])\n",
    "\n",
    "for table in bathy_tables:\n",
    "    data = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    min_lon = data.LON.min()\n",
    "    max_lon = data.LON.max()\n",
    "    min_lat = data.LAT.min()\n",
    "    max_lat = data.LAT.max()\n",
    "    min_lon, min_lat, max_lon, max_lat = inflate_bounds(min_lon, min_lat, max_lon, max_lat, 0.25)\n",
    "    bathy_map = get_map_section(min_lon, max_lon, min_lat, max_lat, 'relief', '15s', \"temp\")\n",
    "    d_bathy = np.hstack([d_bathy, data['DEPTH'] - (-get_map_point(bathy_map, data.LON, data.LAT))])\n",
    "\n",
    "config['bathy_mean_d'] = np.mean(d_bathy, where=~np.isnan(d_bathy))\n",
    "config['bathy_std'] = np.std(d_bathy, where=~np.isnan(d_bathy))\n",
    "\n",
    "plt.hist(d_bathy, bins=100, density=True)\n",
    "plt.xlim([-250,250])\n",
    "plt.xlabel(\"Depth Difference (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bathymetry Difference\")\n",
    "plt.savefig(\".db/plots/bathy_diff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate with velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.particle_filter import process_particle_filter, populate_velocities, plot_error, plot_estimate, summarize_results\n",
    "import json\n",
    "import src.process_dataset as pdset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pdset.get_tables(\".db/parsed.db\")\n",
    "bathy_tables = [table for table in tables if \"_D_\" in table]\n",
    "config = json.load(open(\"config.json\", \"r\"))\n",
    "config[\"n\"] = 1000\n",
    "config[\"cov\"] = [1/60, 1/60, 0, config[\"velocity_noise\"][0], config[\"velocity_noise\"][1], 0]\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'velocity_noise': [2.6, 2.6, 0],\n",
       " 'bathy_mean_d': 21.285045267468078,\n",
       " 'bathy_std': 215.01077619593897,\n",
       " 'n': 1000,\n",
       " 'cov': [0.016666666666666666, 0.016666666666666666, 0, 2.6, 2.6, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdset.table_to_df(\".db/parsed.db\", bathy_tables[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete! Saving results...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('.db/plots2/estimate/'):\n",
    "    os.makedirs('.db/plots2/estimate/')\n",
    "\n",
    "if not os.path.exists('.db/plots2/errors/'):\n",
    "    os.makedirs('.db/plots2/errors/')\n",
    "\n",
    "summary = None\n",
    "annotations = {\"recovery\": 1852, \"res\":1852/4}\n",
    "# for table in tqdm(bathy_tables):\n",
    "#     print(f\"Running {table}\")\n",
    "#     df = pdset.table_to_df(\".db/parsed.db\", table)\n",
    "    \n",
    "df = populate_velocities(df)\n",
    "results, geo_map = process_particle_filter(df, config)\n",
    "print(\"Run complete! Saving results...\")\n",
    "pdset.save_dataset(\n",
    "    [results], [\"test\"], output_location=\".db\", output_format=\"db\", dataset_name=\"results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved! Plotting...\n"
     ]
    }
   ],
   "source": [
    "print(\"Results saved! Plotting...\")\n",
    "fig, ax = plot_estimate(geo_map, results)\n",
    "fig.savefig(f\".db/plots2/estimate/test_estimate.png\")\n",
    "plt.close(fig)\n",
    "fig, ax = plot_error(results, annotations=annotations)\n",
    "fig.savefig(f\".db/plots2/errors/test_error.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "for table in results_tables:\n",
    "    df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    run = summarize_results(df, 1852)\n",
    "    run[\"Name\"] = table\n",
    "    if summary is None:\n",
    "        summary = run#.copy()\n",
    "    else:\n",
    "        summary = pd.concat([summary, run], ignore_index=True)\n",
    "    summary.to_csv(\".db/summary_recovery.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "results_tables = pdset.get_tables(\".db/results.db\")\n",
    "for table in results_tables:\n",
    "    df = pdset.table_to_df(\".db/results.db\", table)\n",
    "    run = summarize_results(df, 452)\n",
    "    run[\"Name\"] = table\n",
    "    if summary is None:\n",
    "        summary = run#.copy()\n",
    "    else:\n",
    "        summary = pd.concat([summary, run], ignore_index=True)\n",
    "    summary.to_csv(\".db/summary_resolution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
